{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guidelines\n",
    "\n",
    "1. Load dataset\n",
    "2. Run parallel representation\n",
    "3. Run AFRN\n",
    "\n",
    "_Note:_ This file has been updated from the Youtube video to reflect the following change in the package:\n",
    "\n",
    "- The AFRN module has been renamed to tGBS \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we'll use the distress dataset that is included in the examples folder of the repository.\n",
    "\n",
    "Textagon requires that the text column in your dataframe has the column name \"corpus\" and the label column has the name \"classLabels\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lalor/miniconda3/envs/textTest2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to /home/lalor/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/lalor/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/lalor/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/lalor/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /home/lalor/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/lalor/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/lalor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/lalor/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textagon.tGBS'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtextagon\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtextagon\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Textagon\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtextagon\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtGBS\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tGBS\n\u001b[32m      5\u001b[39m df = pd.read_csv(\n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msample_data/distress_raw.txt\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m      7\u001b[39m     sep=\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m,     \n\u001b[32m      8\u001b[39m     header=\u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[32m      9\u001b[39m     names=[\u001b[33m\"\u001b[39m\u001b[33mclassLabels\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcorpus\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     10\u001b[39m )\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'textagon.tGBS'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from textagon.textagon import Textagon\n",
    "from textagon.tGBS import tGBS\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"sample_data/distress_raw.txt\", \n",
    "    sep=\"\\t\",     \n",
    "    header=None, \n",
    "    names=[\"classLabels\", \"corpus\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Run parallel representation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Textagon' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m tgon = \u001b[43mTextagon\u001b[49m(\n\u001b[32m      2\u001b[39m     inputFile=df, \n\u001b[32m      3\u001b[39m     outputFileName=\u001b[33m\"\u001b[39m\u001b[33mdistress\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m )\n\u001b[32m      6\u001b[39m tgon.RunFeatureConstruction()\n\u001b[32m      7\u001b[39m tgon.RunPostFeatureConstruction()\n",
      "\u001b[31mNameError\u001b[39m: name 'Textagon' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "tgon = Textagon(\n",
    "    inputFile=df, \n",
    "    outputFileName=\"distress\"\n",
    ")\n",
    "\n",
    "tgon.RunFeatureConstruction()\n",
    "tgon.RunPostFeatureConstruction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Run tGBS\n",
    "\n",
    "In this step we apply tGBS to score and rank the representations. \n",
    "\n",
    "\n",
    "Before running tGBS, we need to unzip the file storing the generated representations. \n",
    "In this case, it's named as \"distress_representations.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files extracted to ./output/distress_representations\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Specify the path to the zip file\n",
    "zip_file_path = './output/distress_representations.zip'\n",
    "\n",
    "# Specify the directory to extract files to\n",
    "extract_to_directory = './output/distress_representations'\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(extract_to_directory, exist_ok=True)\n",
    "\n",
    "# Open the zip file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    # Extract all the contents\n",
    "    zip_ref.extractall(extract_to_directory)\n",
    "\n",
    "print(f\"Files extracted to {extract_to_directory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features\n",
      "0 NA\n",
      "1 BINARY\n",
      "2 CHARBINARY\n",
      "Total categories found =  3\n",
      "Total features found =  163813\n",
      "Total lexicons =  0\n",
      "Loading training data\n",
      "Classes= 2 1 0 Num Instances =  1860\n",
      "Number of features in Features file and Train file are different!!! 163812 163813\n",
      "Loading sentiment scores 4763\n",
      "Loading lexicons...\n",
      "NumLex =  0 NumLexItems =  0\n",
      "Assigning training weights\n",
      "Adding semantic weights\n",
      "0...\n",
      "10000...\n",
      "20000...\n",
      "30000...\n",
      "40000...\n",
      "50000...\n",
      "60000...\n",
      "70000...\n",
      "80000...\n",
      "90000...\n",
      "100000...\n",
      "110000...\n",
      "120000...\n",
      "130000...\n",
      "140000...\n",
      "150000...\n",
      "160000...\n",
      "\n",
      "Running within-category subsumption relations\n",
      "Subsuming category  1  of  3 NA\n",
      "Subsuming category  2  of  3 BINARY\n",
      "Subsuming category  3  of  3 CHARBINARY\n",
      "Running cross-category subsumption relations\n",
      "Running parallel relations\n"
     ]
    }
   ],
   "source": [
    "\n",
    "featuresFile = './output/distress_key.txt'\n",
    "trainFile = './output/distress.csv'\n",
    "weightFile = './output/distress_weights.txt'\n",
    "\n",
    "\n",
    "ranker=tGBS(\n",
    "\tfeaturesFile=featuresFile,\n",
    "\ttrainFile=trainFile,\n",
    "\tweightFile=weightFile\n",
    ")\n",
    "\n",
    "ranker.RankRepresentations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Textagon representations and weights are now stored in the *output* folder, where they can be used for downstream tasks. \n",
    "\n",
    "For two such examples, please look at the other notebooks in the examples folder:\n",
    "\n",
    "- 2-calculate_informativeness.ipynb\n",
    "- 3-classification_with_textagon.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textagon-mUT4rrjR-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
