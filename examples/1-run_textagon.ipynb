{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guidelines\n",
    "\n",
    "1. Load dataset\n",
    "2. Run parallel representation\n",
    "3. Run AFRN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we'll use the distress dataset that is included in the examples folder of the repository.\n",
    "\n",
    "Textagon requires that the text column in your dataframe has the column name \"corpus\" and the label column has the name \"classLabels\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lalor/miniconda3/envs/textTest2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to /home/lalor/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/lalor/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/lalor/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/lalor/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /home/lalor/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/lalor/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/lalor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/lalor/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "current time:- 2025-03-25 09:09:27.640302\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from textagon.textagon import Textagon\n",
    "from textagon.AFRN import AFRN\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"sample_data/distress_raw.txt\", \n",
    "    sep=\"\\t\",     \n",
    "    header=None, \n",
    "    names=[\"classLabels\", \"corpus\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Run parallel representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Spellchecker Details #\n",
      "Provider: <Enchant: Aspell Provider>\n",
      "Enchant Version: 2.3.3\n",
      "Dictionary Tag: en\n",
      "Dictionary Location: /usr/lib/x86_64-linux-gnu/enchant-2/enchant_aspell.so\n",
      "Total Exclusions: 0 (No File Supplied)\n",
      "\n",
      "# CPU Cores Detected and Initialized: 10 #\n",
      "\n",
      "# Python Details #\n",
      "3.11.11 (main, Dec 11 2024, 16:28:39) [GCC 11.2.0] \n",
      "\n",
      "# Package Versions #\n",
      "SpaCy: 3.8.4\n",
      "PyEnchant: 3.2.2\n",
      "pywsd: 1.2.4\n",
      "NLTK: 3.9.1 \n",
      "\n",
      "# Custom Lexicons Imported: 10 #\n",
      "- ADR (4 Tags)\n",
      "- AILEXCAT (81 Tags)\n",
      "- AILEXINT (24 Tags)\n",
      "- EMOLEX (102 Tags)\n",
      "- GLOVECC (500 Tags)\n",
      "- GLOVETW (500 Tags)\n",
      "- GLOVEWG (500 Tags)\n",
      "- LIWC (64 Tags)\n",
      "- SAVLEX (158 Tags)\n",
      "- SYN (500 Tags)\n",
      "\n",
      "# Now Reading Raw Data #\n",
      "Items to Process: 1860 \n",
      "\n",
      "# Now Processing Text Items # \n",
      "\n",
      "# Performing Basic Text Cleanup #\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [01:40<00:00,  5.59s/it]\n",
      "100%|██████████| 18/18 [08:20<00:00, 27.81s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Items Processed: 1860 (Time Elapsed: 0 days 00:10:03)\n",
      "\n",
      "### Stage execution finished at 2025-03-25 09:19 AM EDT (Time Elapsed: 0 days 00:10:22) ###\n",
      "\n",
      "# Now Reading Raw Data #\n",
      "\n",
      "# Now Reading Feature Data Pickle #\n",
      "- Time Elapsed: 0 days 00:00:00\n",
      "\n",
      "# Now Writing Spellchecked Sentences to Disk #\n",
      "- Time Elapsed: 0 days 00:00:00\n",
      "\n",
      "# Now Writing Spelling Corrections to Disk #\n",
      "- Time Elapsed: 0 days 00:00:00\n",
      "\n",
      "# Now Generating VADER Scores #\n",
      "- Time Elapsed: 0 days 00:00:01\n",
      "\n",
      "# Now Constructing Feature Vectors # \n",
      "\n",
      "# Settings #\n",
      "Minimum Term Frequency: 3\n",
      "N-grams: 4\n",
      "Requested Feature Vectors: ['binary', 'charbinary']\n",
      "\n",
      "# Adding Legomena Feature #\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/18 [00:00<?, ?it/s]/home/lalor/miniconda3/envs/textTest2/lib/python3.11/site-packages/textagon/utils.py:279: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  for word in item[0].split(' '):\n",
      "/home/lalor/miniconda3/envs/textTest2/lib/python3.11/site-packages/textagon/utils.py:279: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  for word in item[0].split(' '):\n",
      "/home/lalor/miniconda3/envs/textTest2/lib/python3.11/site-packages/textagon/utils.py:279: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  for word in item[0].split(' '):\n",
      "/home/lalor/miniconda3/envs/textTest2/lib/python3.11/site-packages/textagon/utils.py:279: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  for word in item[0].split(' '):\n",
      "/home/lalor/miniconda3/envs/textTest2/lib/python3.11/site-packages/textagon/utils.py:279: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  for word in item[0].split(' '):\n",
      "/home/lalor/miniconda3/envs/textTest2/lib/python3.11/site-packages/textagon/utils.py:279: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  for word in item[0].split(' '):\n",
      "/home/lalor/miniconda3/envs/textTest2/lib/python3.11/site-packages/textagon/utils.py:279: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  for word in item[0].split(' '):\n",
      "/home/lalor/miniconda3/envs/textTest2/lib/python3.11/site-packages/textagon/utils.py:279: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  for word in item[0].split(' '):\n",
      "/home/lalor/miniconda3/envs/textTest2/lib/python3.11/site-packages/textagon/utils.py:279: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  for word in item[0].split(' '):\n",
      "/home/lalor/miniconda3/envs/textTest2/lib/python3.11/site-packages/textagon/utils.py:279: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  for word in item[0].split(' '):\n",
      "/home/lalor/miniconda3/envs/textTest2/lib/python3.11/site-packages/textagon/utils.py:279: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  for word in item[0].split(' '):\n",
      "/home/lalor/miniconda3/envs/textTest2/lib/python3.11/site-packages/textagon/utils.py:279: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  for word in item[0].split(' '):\n",
      "/home/lalor/miniconda3/envs/textTest2/lib/python3.11/site-packages/textagon/utils.py:279: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  for word in item[0].split(' '):\n",
      "/home/lalor/miniconda3/envs/textTest2/lib/python3.11/site-packages/textagon/utils.py:279: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  for word in item[0].split(' '):\n",
      "  6%|▌         | 1/18 [00:02<00:35,  2.08s/it]/home/lalor/miniconda3/envs/textTest2/lib/python3.11/site-packages/textagon/utils.py:279: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  for word in item[0].split(' '):\n",
      "/home/lalor/miniconda3/envs/textTest2/lib/python3.11/site-packages/textagon/utils.py:279: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  for word in item[0].split(' '):\n",
      "/home/lalor/miniconda3/envs/textTest2/lib/python3.11/site-packages/textagon/utils.py:279: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  for word in item[0].split(' '):\n",
      " 11%|█         | 2/18 [00:03<00:24,  1.52s/it]/home/lalor/miniconda3/envs/textTest2/lib/python3.11/site-packages/textagon/utils.py:279: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  for word in item[0].split(' '):\n",
      "100%|██████████| 18/18 [00:04<00:00,  3.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "# Final Set of Feature Representations (1860 Total) #\n",
      "['Affect', 'Boundaries', 'Hypernym', 'Legomena', 'LexiconADR', 'LexiconAILEXCAT', 'LexiconAILEXINT', 'LexiconEMOLEX', 'LexiconGLOVECC', 'LexiconGLOVETW', 'LexiconGLOVEWG', 'LexiconLIWC', 'LexiconSAVLEX', 'LexiconSYN', 'Misspelling', 'MisspellingDetailed', 'NER', 'POS', 'Sentiment', 'Word', 'Word&NER', 'Word&POS', 'Word&Sense'] \n",
      "\n",
      "# Now Writing Representations to Disk #\n",
      "- Time Elapsed: 0 days 00:00:00\n",
      "\n",
      "# Now Generating Feature Matrices # \n",
      "\n",
      "---\n",
      "Affect\n",
      "Features: 20690 (Time Elapsed: 0 days 00:00:02)\n",
      "---\n",
      "Boundaries\n",
      "Features: 24 (Time Elapsed: 0 days 00:00:00)\n",
      "---\n",
      "Hypernym\n",
      "Features: 19459 (Time Elapsed: 0 days 00:00:01)\n",
      "---\n",
      "Legomena\n",
      "Features: 22439 (Time Elapsed: 0 days 00:00:02)\n",
      "---\n",
      "LexiconADR\n",
      "Features: 20729 (Time Elapsed: 0 days 00:00:02)\n",
      "---\n",
      "LexiconAILEXCAT\n",
      "Features: 20934 (Time Elapsed: 0 days 00:00:04)\n",
      "---\n",
      "LexiconAILEXINT\n",
      "Features: 20978 (Time Elapsed: 0 days 00:00:02)\n",
      "---\n",
      "LexiconEMOLEX\n",
      "Features: 20835 (Time Elapsed: 0 days 00:00:03)\n",
      "---\n",
      "LexiconGLOVECC\n",
      "Features: 18068 (Time Elapsed: 0 days 00:00:01)\n",
      "---\n",
      "LexiconGLOVETW\n",
      "Features: 20017 (Time Elapsed: 0 days 00:00:02)\n",
      "---\n",
      "LexiconGLOVEWG\n",
      "Features: 18388 (Time Elapsed: 0 days 00:00:01)\n",
      "---\n",
      "LexiconLIWC\n",
      "Features: 20094 (Time Elapsed: 0 days 00:00:03)\n",
      "---\n",
      "LexiconSAVLEX\n",
      "Features: 20638 (Time Elapsed: 0 days 00:00:02)\n",
      "---\n",
      "LexiconSYN\n",
      "Features: 20949 (Time Elapsed: 0 days 00:00:02)\n",
      "---\n",
      "Misspelling\n",
      "Features: 20729 (Time Elapsed: 0 days 00:00:03)\n",
      "---\n",
      "MisspellingDetailed\n",
      "Features: 20701 (Time Elapsed: 0 days 00:00:01)\n",
      "---\n",
      "NER\n",
      "Features: 20820 (Time Elapsed: 0 days 00:00:02)\n",
      "---\n",
      "POS\n",
      "Features: 8436 (Time Elapsed: 0 days 00:00:00)\n",
      "---\n",
      "Sentiment\n",
      "Features: 22159 (Time Elapsed: 0 days 00:00:02)\n",
      "---\n",
      "Word\n",
      "Features: 20690 (Time Elapsed: 0 days 00:00:01)\n",
      "---\n",
      "Word&NER\n",
      "Features: 20617 (Time Elapsed: 0 days 00:00:01)\n",
      "---\n",
      "Word&POS\n",
      "Features: 20612 (Time Elapsed: 0 days 00:00:02)\n",
      "---\n",
      "Word&Sense\n",
      "Features: 18896 (Time Elapsed: 0 days 00:00:01)\n",
      "\n",
      "# Adding Character N-grams (charbinary-Word) #\n",
      "Features: 12813 (Time Elapsed: 0 days 00:00:02)\n",
      "\n",
      "# Now Joining Feature Matrices # \n",
      "\n",
      "Processed distress_binary_Word_feature_matrix (Time Elapsed: 0 days 00:00:00)\n",
      "Processed distress_binary_Affect_feature_matrix (Time Elapsed: 0 days 00:00:03)\n",
      "Processed distress_binary_Boundaries_feature_matrix (Time Elapsed: 0 days 00:00:02)\n",
      "Processed distress_binary_Hypernym_feature_matrix (Time Elapsed: 0 days 00:00:03)\n",
      "Processed distress_binary_Legomena_feature_matrix (Time Elapsed: 0 days 00:00:04)\n",
      "Processed distress_binary_LexiconADR_feature_matrix (Time Elapsed: 0 days 00:00:04)\n",
      "Processed distress_binary_LexiconAILEXCAT_feature_matrix (Time Elapsed: 0 days 00:00:05)\n",
      "Processed distress_binary_LexiconAILEXINT_feature_matrix (Time Elapsed: 0 days 00:00:06)\n",
      "Processed distress_binary_LexiconEMOLEX_feature_matrix (Time Elapsed: 0 days 00:00:06)\n",
      "Processed distress_binary_LexiconGLOVECC_feature_matrix (Time Elapsed: 0 days 00:00:07)\n",
      "Processed distress_binary_LexiconGLOVETW_feature_matrix (Time Elapsed: 0 days 00:00:07)\n",
      "Processed distress_binary_LexiconGLOVEWG_feature_matrix (Time Elapsed: 0 days 00:00:07)\n",
      "Processed distress_binary_LexiconLIWC_feature_matrix (Time Elapsed: 0 days 00:00:09)\n",
      "Processed distress_binary_LexiconSAVLEX_feature_matrix (Time Elapsed: 0 days 00:00:09)\n",
      "Processed distress_binary_LexiconSYN_feature_matrix (Time Elapsed: 0 days 00:00:10)\n",
      "Processed distress_binary_Misspelling_feature_matrix (Time Elapsed: 0 days 00:00:10)\n",
      "Processed distress_binary_MisspellingDetailed_feature_matrix (Time Elapsed: 0 days 00:00:12)\n",
      "Processed distress_binary_NER_feature_matrix (Time Elapsed: 0 days 00:00:12)\n",
      "Processed distress_binary_POS_feature_matrix (Time Elapsed: 0 days 00:00:14)\n",
      "Processed distress_binary_Sentiment_feature_matrix (Time Elapsed: 0 days 00:00:16)\n",
      "Processed distress_binary_Word_NER_feature_matrix (Time Elapsed: 0 days 00:00:12)\n",
      "Processed distress_binary_Word_POS_feature_matrix (Time Elapsed: 0 days 00:00:14)\n",
      "Processed distress_binary_Word_Sense_feature_matrix (Time Elapsed: 0 days 00:00:14)\n",
      "Processed distress_charbinary_Word_feature_matrix (Time Elapsed: 0 days 00:00:14)\n",
      "\n",
      "Number of Features Produced: 450715 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:08<00:00,  9.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Zero Variance Features Removed: 23 (Time Elapsed: 0 days 00:00:19)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:15<00:00,  5.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Duplicate Features Removed: 286887 (Time Elapsed: 0 days 00:00:25)\n",
      "\n",
      "# Now Writing Results to Disk #\n",
      "- Time Elapsed: 0 days 00:00:56\n",
      "\n",
      "# Now Generating Column Key Files #\n",
      "- Time Elapsed: 0 days 00:00:03\n",
      "\n",
      "Output Dimensions (Rows, Features): (1860, 163813) \n",
      "\n",
      "### Execution finished at 2025-03-25 09:25 AM EDT (Time Elapsed: 0 days 00:16:28) ###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tgon = Textagon(\n",
    "    inputFile=df, \n",
    "    outputFileName=\"distress\",\n",
    "    maxCores=10\n",
    ")\n",
    "\n",
    "tgon.RunFeatureConstruction()\n",
    "tgon.RunPostFeatureConstruction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Run AFRN\n",
    "\n",
    "Before running AFRN, we need to unzip the file storing the generated representations. \n",
    "In this case, it's named as \"distress_representations.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files extracted to ./output/distress_representations\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Specify the path to the zip file\n",
    "zip_file_path = './output/distress_representations.zip'\n",
    "\n",
    "# Specify the directory to extract files to\n",
    "extract_to_directory = './output/distress_representations'\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(extract_to_directory, exist_ok=True)\n",
    "\n",
    "# Open the zip file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    # Extract all the contents\n",
    "    zip_ref.extractall(extract_to_directory)\n",
    "\n",
    "print(f\"Files extracted to {extract_to_directory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features\n",
      "0 NA\n",
      "1 BINARY\n",
      "2 CHARBINARY\n",
      "Total categories found =  3\n",
      "Total features found =  163813\n",
      "Total lexicons =  0\n",
      "Loading training data\n",
      "Classes= 2 1 0 Num Instances =  1860\n",
      "Number of features in Features file and Train file are different!!! 163812 163813\n",
      "Loading sentiment scores 4763\n",
      "Loading lexicons...\n",
      "NumLex =  0 NumLexItems =  0\n",
      "Assigning training weights\n",
      "Adding semantic weights\n",
      "0...\n",
      "10000...\n",
      "20000...\n",
      "30000...\n",
      "40000...\n",
      "50000...\n",
      "60000...\n",
      "70000...\n",
      "80000...\n",
      "90000...\n",
      "100000...\n",
      "110000...\n",
      "120000...\n",
      "130000...\n",
      "140000...\n",
      "150000...\n",
      "160000...\n",
      "\n",
      "Running within-category subsumption relations\n",
      "Subsuming category  1  of  3 NA\n",
      "Subsuming category  2  of  3 BINARY\n",
      "Subsuming category  3  of  3 CHARBINARY\n",
      "Running cross-category subsumption relations\n",
      "Running parallel relations\n"
     ]
    }
   ],
   "source": [
    "\n",
    "featuresFile = './output/distress_key.txt'\n",
    "trainFile = './output/distress.csv'\n",
    "weightFile = './output/distress_weights.txt'\n",
    "\n",
    "\n",
    "afrn=AFRN(\n",
    "\tfeaturesFile=featuresFile,\n",
    "\ttrainFile=trainFile,\n",
    "\tweightFile=weightFile\n",
    ")\n",
    "\n",
    "afrn.RankRepresentations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Textagon representations and weights are now stored in the *output* folder, where they can be used for downstream tasks. \n",
    "\n",
    "For two such examples, please look at the other notebooks in the examples folder:\n",
    "\n",
    "- 2-calculate_informativeness.ipynb\n",
    "- 3-classification_with_textagon.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textTest2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
