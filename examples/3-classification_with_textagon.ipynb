{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9u8CAjpTac5Z"
   },
   "source": [
    "# Textagon System Demonstration\n",
    "\n",
    "This Colab file demonstrates the functionality of the Textagon software.\n",
    "Because of Google Colab resource constraints and RAM limitations, in certain spots, we rely on pre-processed portions of the Textagon pipeline.\n",
    "In those places, we include the command that was run offline to generate the preprocessed component.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfbvsNi8D74l"
   },
   "source": [
    "##### Arguments:\n",
    "1. inputFileFullPath: Path to the input text file\n",
    "2. outputFileName: Name for the output file\n",
    "3. inputLimit: Limit on the number of input files (0 means no limit)\n",
    "4. maxFeatures: Maximum number of features to extract (0 means no limit)\n",
    "5. maxNgram: Maximum n-gram size (1 for unigrams, 2 for bigrams, etc.)\n",
    "6. maxCores: Number of CPU cores to use\n",
    "7. lexiconFileFullPath: Path to the lexicon file (zip or folder containing .txt files)\n",
    "8. vader: Whether to use VADER sentiment analysis (1 for true, 0 for false)\n",
    "9. wnaReturnLevel: WNA return level\n",
    "10. buildVectors: Build vectors setting (custom value, e.g., 'bB')\n",
    "11. index: Whether to index the output (1 for true, 0 for false)\n",
    "12. removeZeroVariance: Remove zero variance features (1 for true, 0 for false)\n",
    "13. combineFeatures: Combine features (1 for true, 0 for false)\n",
    "14. minDF: Minimum document frequency (integer or float value)\n",
    "15. removeDupColumns: Remove duplicate columns (1 for true, 0 for false)\n",
    "16. useSpellChecker: Use spell checker (1 for true, 0 for false)\n",
    "17. provideMisspellingDetailed: Provide detailed misspelling information (1 for true, 0 for false)\n",
    "18. additionalCols: Include additional columns in the output (1 for true, 0 for false)\n",
    "19. writeRepresentations: Write representations to the output (1 for true, 0 for false)\n",
    "20. exclusionsFileFullPath: Path to the exclusions file\n",
    "21. runType: Type of run (e.g., 'full', 'partial')\n",
    "\n",
    "```python\n",
    "mkdir -p content/output\n",
    "python process-text.py \\\n",
    "    ./sample_data_1/textagon_sample/raw.txt \\\n",
    "    ./sample_data_1/textagon_sample \\\n",
    "    0 0 1 4 external/lexicons/Lexicons_v5.zip 1 5 bB 0 1 0 3 1 1 0 1 1 upload/exclusions.txt full\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J99U4PCoD74m"
   },
   "source": [
    "#### 5. Compare BERT only and BERT with textagon\n",
    "\n",
    "After you have generated the parallel representations, you can plug them into a BERT classifier pipeline.\n",
    "\n",
    "**Note**: The below section of code takes several hours to run.\n",
    "The results are included in the manuscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 08:57:55.407020: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-25 08:57:55.417481: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-25 08:57:55.427097: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742907475.441392  990753 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742907475.445500  990753 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1742907475.459866  990753 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742907475.459882  990753 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742907475.459884  990753 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742907475.459885  990753 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-25 08:57:55.464210: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#from datasets import load_dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "import six\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "#from huggingface_hub import HfApi\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "import six\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "import six\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import os, warnings, torch, nltk, csv, logging\n",
    "from keras import *\n",
    "from gensim.models import *\n",
    "from sklearn.metrics import *\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing import *\n",
    "from numpy.random import seed\n",
    "seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/crc.nd.edu/user/j/jlalor1/.conda/envs/textagon/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### For wordcnn ###\n",
    "from torch.nn import functional as F\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "if not nltk.data.find('tokenizers/punkt'): nltk.download('punkt')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # device\n",
    "\n",
    "### For bert ###\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"transformers.configuration_utils\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)\n",
    "\n",
    "# from utils import split_into_train_valid_test, save_plot_and_metric, load_or_make\n",
    "# from utils import get_prediction_E2E, data_wordcnn_E2E\n",
    "# from utils import set_bert_model, MLP\n",
    "\n",
    "import string\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import roc_auc_score, f1_score, recall_score, precision_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import collections\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from torch.nn.functional import softmax\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertConfig, BertTokenizer, BertModel, BertForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    return ''.join([ch.lower() for ch in text if ch not in string.punctuation])\n",
    "\n",
    "class Dataset_bert(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, truncation=True, max_length=self.max_length, padding='max_length', return_tensors='pt')\n",
    "        return {'input_ids': encoding['input_ids'].squeeze(),\n",
    "                'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "                'labels': torch.tensor(label).long()}\n",
    "\n",
    "def process_bert_inputs(texts, tokenizer, max_length):\n",
    "    encodings = [tokenizer(text, truncation=True, max_length=max_length, padding='max_length') for text in texts]\n",
    "    input_ids = torch.tensor([enc['input_ids'] for enc in encodings], dtype=torch.long)\n",
    "    attention_masks = torch.tensor([enc['attention_mask'] for enc in encodings], dtype=torch.long)\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "def fbeta_score(y_true, y_pred, beta):\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    return (1 + beta**2) * precision * recall / (beta**2 * precision + recall)\n",
    "\n",
    "def process_data_for_transformer(data_path, test_size, tokenizer, max_length, batch_size):\n",
    "    data = open(data_path, 'r', errors='replace').readlines()\n",
    "    labels = [int(row.strip().split('\\t')[0]) for row in data]\n",
    "    texts = [row.strip().split('\\t')[1] for row in data]\n",
    "    train_text, test_text, training_label, testing_label = train_test_split(texts, labels, test_size=test_size, random_state=42)\n",
    "    test_text, void_text, testing_label, void_label = train_test_split(test_text, testing_label, test_size=0.5, random_state=42)\n",
    "    train_dataset_bt = Dataset_bert(train_text, training_label, tokenizer, max_length)\n",
    "    val_dataset_bt = Dataset_bert(test_text, testing_label, tokenizer, max_length)\n",
    "    train_loader_bt = DataLoader(train_dataset_bt, batch_size=batch_size, shuffle=False)\n",
    "    val_loader_bt = DataLoader(val_dataset_bt, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader_bt, val_loader_bt\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, feature_extractor, cls_emb_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.transformer = feature_extractor\n",
    "        self.classifier = nn.Linear(cls_emb_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.transformer(input_ids, attention_mask=attention_mask)\n",
    "        # using pooled_output corresponding to the [CLS] token\n",
    "        if isinstance(outputs, tuple):\n",
    "            pooled_output = outputs[1]  # assuming output_hidden_states and output_attentions are False\n",
    "        else:\n",
    "            pooled_output = outputs.pooler_output\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def get_full_combos(lst, max_length=None):\n",
    "    \"\"\"\n",
    "    Generate combinations of elements from a list, ensuring the first element is always included.\n",
    "\n",
    "    Args:\n",
    "    - lst (list): A list of elements.\n",
    "    - max_length (int, optional): The maximum allowed length for the resulting combinations.\n",
    "                                  If not provided, combinations of all lengths are allowed.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of combinations. Each combination is a list of elements.\n",
    "\n",
    "    Example:\n",
    "        lst = [1, 2, 3, 4]\n",
    "        print(get_full_combos(lst))\n",
    "        Output:\n",
    "        [[1],\n",
    "         [2],\n",
    "         [3],\n",
    "         [4],\n",
    "         [1, 2],\n",
    "         [1, 3],\n",
    "         [1, 4],\n",
    "         [1, 2, 3],\n",
    "         [1, 2, 4],\n",
    "         [1, 3, 4],\n",
    "         [1, 2, 3, 4]]\n",
    "\n",
    "        print(get_full_combos(lst, max_length=2))\n",
    "        Output:\n",
    "        [[1],\n",
    "         [2],\n",
    "         [3],\n",
    "         [4],\n",
    "         [1, 2],\n",
    "         [1, 3],\n",
    "         [1, 4]]\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for i in range(1, len(lst) + 1):\n",
    "        for combo in combinations(lst, i):\n",
    "            # If max_length is provided, ensure the combo length does not exceed it\n",
    "            if max_length and len(combo) > max_length:\n",
    "                continue\n",
    "            result.append(list(combo))\n",
    "    return result\n",
    "\n",
    "def get_rep_linear_combos(lst):\n",
    "    result = []\n",
    "    for i in range(1, len(lst) + 1):\n",
    "        result.append(list(lst[:i]))\n",
    "    return result\n",
    "\n",
    "def process_texts_E2E(texts, word_to_ix, max_sentence_length):\n",
    "    processed = [simple_tokenize(text) for text in texts]\n",
    "    processed = [[word_to_ix[word] if word in word_to_ix else word_to_ix['<UNK>'] for word in text] for text in processed]\n",
    "    processed = [text[:max_sentence_length] + [word_to_ix['<PAD>']]*(max_sentence_length - len(text)) for text in processed]\n",
    "    return torch.tensor(processed, dtype=torch.long)\n",
    "\n",
    "def data_wordcnn_E2E(trainText, validationText, testText, trainingLabels, validationLabels, testLabels, word_to_ix, batch_size, MAX_SENTENCE_LENGTH):\n",
    "\n",
    "    train_data = process_texts_E2E(trainText, word_to_ix, MAX_SENTENCE_LENGTH)\n",
    "    train_labels = torch.tensor(trainingLabels, dtype=torch.long)\n",
    "\n",
    "    validation_data = process_texts_E2E(validationText, word_to_ix, MAX_SENTENCE_LENGTH)\n",
    "    validation_labels = torch.tensor(validationLabels, dtype=torch.long)\n",
    "\n",
    "    test_data = process_texts_E2E(testText, word_to_ix, MAX_SENTENCE_LENGTH)\n",
    "    test_labels = torch.tensor(testLabels, dtype=torch.long)\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(train_data, train_labels)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    validation_dataset = torch.utils.data.TensorDataset(validation_data, validation_labels)\n",
    "    validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    test_dataset = torch.utils.data.TensorDataset(test_data, test_labels)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, validation_loader, test_loader\n",
    "\n",
    "\n",
    "def create_3d_loader(loader_list, shuffle=False):\n",
    "    data_list, label_list = [], []\n",
    "    for data_and_labels in zip(*loader_list):\n",
    "        data = torch.cat([item[0].unsqueeze(1) for item in data_and_labels], dim=1)\n",
    "        label = data_and_labels[0][1]  # assuming the labels are the same across all loaders\n",
    "        data_list.append(data)\n",
    "        label_list.append(label)\n",
    "    new_loader = list(zip(data_list, label_list))\n",
    "\n",
    "    if shuffle:\n",
    "        random.shuffle(new_loader)\n",
    "\n",
    "    return new_loader\n",
    "\n",
    "class CNN2D_E2E_AE(nn.Module):\n",
    "    def __init__(self, vocab_size, num_filters, num_class, num_channels, width, auto_encoder_path):\n",
    "        super(CNN2D_E2E_AE, self).__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, width)\n",
    "\n",
    "        self.autoencoder = Autoencoder(vocab_size, width)\n",
    "        self.autoencoder.load_state_dict(torch.load(f'{auto_encoder_path}/autoencoder.pth'))  # Load trained weights\n",
    "        self.autoencoder = self.autoencoder.encoder\n",
    "\n",
    "        conv_dim = int(width * 2/3)\n",
    "        self.conv1_1 = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, num_filters, kernel_size=(1, conv_dim // 6), stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(1, conv_dim - (conv_dim // 6) + 1))\n",
    "        ).cuda()\n",
    "        self.conv1_2 = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, num_filters, kernel_size=(2, conv_dim // 4), stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(1, conv_dim - (conv_dim // 4) + 1))\n",
    "        ).cuda()\n",
    "        self.conv1_3 = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, num_filters, kernel_size=(3, conv_dim // 3), stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(1, conv_dim - (conv_dim // 3) + 1))\n",
    "        ).cuda()\n",
    "        self.fc = nn.Linear(num_filters, num_class)\n",
    "        self.adaptive_pool = nn.AdaptiveMaxPool2d((None, 1))\n",
    "\n",
    "    def forward(self, x, output_last_layer=False):\n",
    "        # print('0:', x.shape) # torch.Size([8, 1, 500])\n",
    "        x = x.long() # Make sure x is of type LongTensor for Embedding Layer\n",
    "        x = self.embed(x)\n",
    "        # print('1:', x.shape) # torch.Size([8, 1, 500, 500])\n",
    "        x = self.autoencoder(x)\n",
    "        # print('2:', x.shape) # torch.Size([8, 1, 500, 333])\n",
    "        # x = x.unsqueeze(1) # [batch_size, 1, num_models, seq_length, emb_dim]\n",
    "        c1 = self.conv1_1(x).squeeze(-1)\n",
    "        # print('c1:', c1.shape)\n",
    "        c2 = self.conv1_2(x).squeeze(-1)\n",
    "        c3 = self.conv1_3(x).squeeze(-1)\n",
    "        # print(\"3:\", c1.shape, c2.shape, c3.shape) # torch.Size([8, 600, 500]) torch.Size([8, 600, 499]) torch.Size([8, 600, 498])\n",
    "        c = torch.cat([c1, c2, c3], dim = -1)\n",
    "        # print(\"4:\", c.shape) # torch.Size([8, 600, 1497])\n",
    "        c = self.adaptive_pool(c).squeeze(-1)\n",
    "        # print(\"5:\", c.shape) # torch.Size([8, 600])\n",
    "        c = c.view(c.size(0), -1)  # Flatten tensor while preserving the batch size\n",
    "        # print(\"6:\", c.shape)\n",
    "        if output_last_layer:\n",
    "            return c\n",
    "        else:\n",
    "            logit = self.fc(c)\n",
    "            return logit\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(emb_dim, int(emb_dim * (4/5))),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(int(emb_dim * (4/5)), int(emb_dim * (3/4))),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(int(emb_dim * (3/4)), int(emb_dim * (2/3))),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(int(emb_dim * (2/3)), int(emb_dim * (3/4))),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(int(emb_dim * (3/4)), int(emb_dim * (4/5))),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(int(emb_dim * (4/5)), emb_dim),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_encoded = self.encoder(x)\n",
    "        x_decoded = self.decoder(x_encoded)\n",
    "        return x_decoded\n",
    "\n",
    "def fbeta_score(y_true, y_pred, beta):\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    return (1 + beta**2) * precision * recall / (beta**2 * precision + recall)\n",
    "\n",
    "def check_loader_shapes(loader_list):\n",
    "    shape_dict = {}\n",
    "    for i, dataloader in enumerate(loader_list):\n",
    "        for j, (input, label) in enumerate(dataloader):\n",
    "            if j not in shape_dict:\n",
    "                shape_dict[j] = input.shape\n",
    "            elif shape_dict[j] != input.shape:\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "def simple_tokenize(text):\n",
    "    return text.split()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JylKZ6HJD74m"
   },
   "source": [
    "##### Run BERT only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "bROfn79WD74n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats: \n",
      "Train Loss: 0.6879, Acc: 0.5568, AUC: 0.5567, Recall: 0.5533, F1: 0.5467, F2: 0.5506\n",
      "Test Loss: 0.6827, Acc: 0.5878, AUC: 0.5872, Recall: 0.4029, F1: 0.4934, F2: 0.4348\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "original_data_path = f'./sample_data'\n",
    "\n",
    "\"\"\"Load parameters\"\"\"\n",
    "num_classes = 2  # number of output classes\n",
    "###### Bert ######\n",
    "lr_bert = 1e-5\n",
    "batch_size_bt = 128\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "max_length_bert = 128\n",
    "epochs = 5\n",
    "epochs = 1\n",
    "test_data = 0.3\n",
    "\n",
    "train_loader_transformer, test_loader_transformer =  process_data_for_transformer(\n",
    "    data_path = f'{original_data_path}/distress_raw.txt',\n",
    "    test_size = test_data,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length = max_length_bert,\n",
    "    batch_size=batch_size_bt\n",
    ")\n",
    "\n",
    "config_bert = BertConfig.from_pretrained('bert-base-uncased', num_labels=num_classes)\n",
    "model_bert = TransformerClassifier(\n",
    "        feature_extractor = BertModel.from_pretrained('bert-base-uncased'),\n",
    "        cls_emb_size = config_bert.hidden_size,\n",
    "        num_classes = num_classes).to(device)\n",
    "optimizer_bert = torch.optim.AdamW(model_bert.parameters(), lr=lr_bert)\n",
    "\n",
    "best_auc = 0.0\n",
    "best_preds, best_labels = None, None\n",
    "for epoch in range(epochs):\n",
    "    # log_file = open(f'{save_batch_path}/log.txt', 'a')\n",
    "    model_bert.train()\n",
    "    train_loss = 0\n",
    "    train_preds, train_targets = [], []\n",
    "    for inputs_trans in train_loader_transformer:\n",
    "        optimizer_bert.zero_grad()\n",
    "        labels = inputs_trans['labels'].to(device)\n",
    "        inputs_trans, attention_mask_trans = inputs_trans['input_ids'].to(device), inputs_trans['attention_mask'].to(device)\n",
    "        outputs_trans = model_bert(inputs_trans, attention_mask=attention_mask_trans)\n",
    "        loss = F.cross_entropy(outputs_trans, labels)\n",
    "        loss.backward()\n",
    "        optimizer_bert.step()\n",
    "        train_loss += loss.item()\n",
    "        preds = torch.argmax(outputs_trans, dim=-1)\n",
    "        train_preds.extend(preds.cpu().numpy())\n",
    "        train_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "    train_loss /= len(train_loader_transformer)  # average loss over the epoch\n",
    "    train_acc = (np.array(train_preds) == np.array(train_targets)).mean()  # accuracy\n",
    "    train_auc = roc_auc_score(train_targets, train_preds)  # AUC\n",
    "\n",
    "    train_recall = recall_score(train_targets, train_preds)  # recall\n",
    "    train_f1 = f1_score(train_targets, train_preds)  # F1\n",
    "    train_f2 = fbeta_score(train_targets, train_preds, beta=2)  # F2\n",
    "\n",
    "    model_bert.eval()\n",
    "    test_loss = 0\n",
    "    test_preds, test_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs_trans in test_loader_transformer:\n",
    "            labels = inputs_trans['labels'].to(device)\n",
    "            inputs_trans, attention_mask_trans = inputs_trans['input_ids'].to(device), inputs_trans['attention_mask'].to(device)\n",
    "            outputs_trans = model_bert(inputs_trans, attention_mask=attention_mask_trans)\n",
    "            loss = F.cross_entropy(outputs_trans, labels)\n",
    "            test_loss += loss.item()\n",
    "            preds = torch.argmax(outputs_trans, dim=-1)\n",
    "            test_preds.extend(preds.cpu().numpy())\n",
    "            test_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "    test_loss /= len(test_loader_transformer)  # average loss over the epoch\n",
    "    test_acc = (np.array(test_preds) == np.array(test_targets)).mean()  # accuracy\n",
    "    test_auc = roc_auc_score(test_targets, test_preds)  # AUC\n",
    "    test_recall = recall_score(test_targets, test_preds)  # recall\n",
    "    test_f1 = f1_score(test_targets, test_preds)  # F1\n",
    "    test_f2 = fbeta_score(test_targets, test_preds, beta=2)  # F2\n",
    "\n",
    "stats = f'Stats: \\n' \\\n",
    "            f'Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, AUC: {train_auc:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f}, F2: {train_f2:.4f}\\n' \\\n",
    "            f'Test Loss: {test_loss:.4f}, Acc: {test_acc:.4f}, AUC: {test_auc:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}, F2: {test_f2:.4f}\\n\\n'\n",
    "print(stats)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmUkZ9sVD74n"
   },
   "source": [
    "##### Run BERT with textagon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uL_24Ul0D74n"
   },
   "outputs": [],
   "source": [
    "# JL\n",
    "### MAYBE SKIP THIS CELL??? ###\n",
    "\n",
    "\"\"\"load GBS weights from excel file\"\"\"\n",
    "\n",
    "textagon_path = './textagon.xlsx'\n",
    "df = pd.read_excel(textagon_path)\n",
    "df = df.drop(df.columns[0], axis=1)\n",
    "\n",
    "features = df.iloc[0, 1:].tolist()\n",
    "result_dict = {}\n",
    "\n",
    "drop_features = []\n",
    "\n",
    "# Iterating from the second row onwards\n",
    "for index, row in df.iloc[1:].iterrows():\n",
    "    dataset_name = row[0]\n",
    "    paired = list(zip(features, row[1:]))  # Pair each feature with its value\n",
    "\n",
    "    # Filter out the undesired features\n",
    "    # paired = [pair for pair in paired if pair[0] not in drop_features]\n",
    "\n",
    "    paired_sorted = sorted(paired, key=lambda x: x[1], reverse=True)  # Sort pairs based on the value\n",
    "    sorted_features, sorted_values = zip(*paired_sorted)  # Separate the pairs back into lists\n",
    "    result_dict[dataset_name] = (list(sorted_features), list(sorted_values))  # Save into the dictionary\n",
    "\n",
    "    # Move \"Word\" to the front\n",
    "    if \"Word\" in sorted_features:\n",
    "        word_index = sorted_features.index(\"Word\")\n",
    "        sorted_features = [\"Word\"] + [f for i, f in enumerate(sorted_features) if i != word_index]\n",
    "        word_value = sorted_values[word_index]\n",
    "        sorted_values = [word_value] + [v for i, v in enumerate(sorted_values) if i != word_index]\n",
    "\n",
    "    result_dict[dataset_name] = (list(sorted_features), list(sorted_values))  # Save into the dictionary\n",
    "\n",
    "# print(result_dict['Anxiety'][0])\n",
    "# print(result_dict['Anxiety'][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cn7iIVkxD74n"
   },
   "outputs": [],
   "source": [
    "'''This is pick the accumulatively lowest cosine similarity between WORD representations and others'''\n",
    "\n",
    "representations_folder = f'./output/distress_representations'\n",
    "representation_list = os.listdir(representations_folder)\n",
    "for rep_temp in representation_list:\n",
    "    ## Rename the file\n",
    "    new_name = rep_temp.split('_')[-1]\n",
    "    os.rename(f'{representations_folder}/{rep_temp}', f'{representations_folder}/{new_name}')\n",
    "\n",
    "\n",
    "###### helper functions ######\n",
    "def simple_tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "def process_texts_E2E(texts, word_to_ix, max_sentence_length):\n",
    "    processed = [simple_tokenize(text) for text in texts]\n",
    "    processed = [[word_to_ix[word] if word in word_to_ix else word_to_ix['<UNK>'] for word in text] for text in processed]\n",
    "    processed = [text + [word_to_ix['<PAD>']] * (max_sentence_length - len(text)) for text in processed]\n",
    "    return torch.tensor(processed, dtype=torch.long)\n",
    "\n",
    "###### Get the expressive scores ######\n",
    "textagon_path = './textagon.xlsx'\n",
    "df = pd.read_excel(textagon_path)\n",
    "df = df.drop(df.columns[0], axis=1)\n",
    "features = df.iloc[0, 1:].tolist()\n",
    "result_dict = {}\n",
    "\n",
    "for index, row in df.iloc[1:].iterrows():\n",
    "    dataset_name = row[0]\n",
    "    paired = list(zip(features, row[1:]))\n",
    "    paired_sorted = sorted(paired, key=lambda x: x[1], reverse=True)\n",
    "    sorted_features, sorted_values = zip(*paired_sorted)\n",
    "    result_dict[dataset_name] = (list(sorted_features), list(sorted_values))\n",
    "\n",
    "###### Get embeddings for datasets ######\n",
    "# # trans_model = 'final_v2_17_distilbert'\n",
    "# datasets = os.listdir(data_folder)\n",
    "# datasets = [d for d in datasets if '.' not in d]\n",
    "representations = os.listdir(representations_folder)\n",
    "high_path = './data/bs'\n",
    "dataset_dict = dict()\n",
    "\n",
    "\n",
    "master_vocab = set()\n",
    "for rep in representations:\n",
    "    rep = rep.split('_')[-1]\n",
    "    if rep == 'Boundaries.txt': continue\n",
    "    texts = open(f'{representations_folder}/{rep}', 'r', errors='replace').readlines()\n",
    "    texts_tokened = [simple_tokenize(text) for text in texts]\n",
    "    for sentence in texts_tokened:\n",
    "        master_vocab.update(sentence)\n",
    "\n",
    "original_tokens = sorted([word for word in master_vocab if not word.isupper()], key=lambda x: (not x.isalpha(), x))\n",
    "replaced_tokens = sorted([word for word in master_vocab if word.isupper()])\n",
    "word_to_ix = {'<UNK>': 0, '<PAD>': 1}\n",
    "for i, word in enumerate(original_tokens + replaced_tokens, start=2):\n",
    "    word_to_ix[word] = i\n",
    "\n",
    "length_list = []\n",
    "for rep in representations:\n",
    "    rep = rep.split('_')[-1]\n",
    "    if rep == 'Boundaries.txt': continue\n",
    "    texts = open(f'{representations_folder}/{rep}', 'r', errors='replace').readlines()\n",
    "    texts_tokened = [simple_tokenize(text) for text in texts]\n",
    "    max_sentence_length = max(len(sentence) for sentence in texts_tokened)\n",
    "    length_list.append(max_sentence_length)\n",
    "length = max(length_list)\n",
    "\n",
    "rep_dict = dict()\n",
    "for rep in representations:\n",
    "    rep = rep.split('_')[-1]\n",
    "    if rep == 'Boundaries.txt': continue\n",
    "    texts = open(f'{representations_folder}/{rep}', 'r', errors='replace').readlines()\n",
    "    rep_dict[rep.split('.')[0]] = process_texts_E2E(texts=texts, word_to_ix=word_to_ix, max_sentence_length=length)\n",
    "dataset_dict['distress'] = (rep_dict, word_to_ix, length)\n",
    "\n",
    "\n",
    "def cosine_similarity_v0(matrix1, matrix2):\n",
    "    flattened1 = matrix1.float().flatten()\n",
    "    flattened2 = matrix2.float().flatten()\n",
    "    return F.cosine_similarity(flattened1.unsqueeze(0), flattened2.unsqueeze(0))\n",
    "\n",
    "\n",
    "def get_highest_expressive_power_rep(dataset, rank=1):\n",
    "    all_reps = result_dict[dataset][0]\n",
    "    expressive_values = result_dict[dataset][1]\n",
    "\n",
    "    # Filter out 'Word'\n",
    "    reps_filtered = [rep for rep in all_reps if rep != 'Word']\n",
    "    values_filtered = [expressive_values[all_reps.index(rep)] for rep in reps_filtered]\n",
    "\n",
    "    sorted_indices = sorted(range(len(values_filtered)), key=lambda k: values_filtered[k], reverse=True)\n",
    "    return reps_filtered[sorted_indices[rank-1]]\n",
    "\n",
    "\n",
    "def get_lowest_cosine_similarity_rep(dataset, rank=1):\n",
    "    word_rep = dataset_dict[dataset][0]['Word']\n",
    "    top_7_reps = result_dict[dataset][0][:7]\n",
    "    similarities = {}\n",
    "    for rep in top_7_reps:\n",
    "        if rep != 'Word':\n",
    "            similarities[rep] = cosine_similarity_v0(word_rep, dataset_dict[dataset][0][rep]).item()\n",
    "    sorted_reps = sorted(similarities, key=similarities.get)\n",
    "    return sorted_reps[rank-1]\n",
    "\n",
    "def get_best_auc_for_epoch(selected_epoch, path = './data/bs'):\n",
    "    # dataset = 'distress'\n",
    "    datasets = ['distress']\n",
    "    optimal_solutions = dict()\n",
    "\n",
    "    for dataset in datasets:\n",
    "        # rep_dict = dict()\n",
    "        # for rep_combination in os.listdir(f'{path}/{dataset}'):\n",
    "        #     rep_dict[frozenset(rep_combination.split('_'))] = rep_combination\n",
    "\n",
    "        # print(f\"\\nFor dataset {dataset}:\")\n",
    "\n",
    "        comb_names = []\n",
    "        temp_names = []\n",
    "\n",
    "        # 1. \"Word\"\n",
    "        comb_names.append('Word')\n",
    "\n",
    "        # 2. Highest expressive power representation (exp 1)\n",
    "        exp1 = get_highest_expressive_power_rep(dataset, 1)\n",
    "        comb_names.append(f\"{exp1}\")\n",
    "\n",
    "        # 3. Second highest expressive power representation (exp 2)\n",
    "        exp2 = get_highest_expressive_power_rep(dataset, 2)\n",
    "        comb_names.append(f\"{exp2}\")\n",
    "\n",
    "        # 3-1. Third highest expressive power representation (exp 2)\n",
    "        exp3 = get_highest_expressive_power_rep(dataset, 3)\n",
    "        comb_names.append(f\"{exp3}\")\n",
    "\n",
    "        # 4. & 5. Lowest and Second Lowest cosine similarity\n",
    "        cos1 = get_lowest_cosine_similarity_rep(dataset, 1)\n",
    "        comb_names.append(f\"{cos1}\")\n",
    "\n",
    "        cos2 = get_lowest_cosine_similarity_rep(dataset, 2)\n",
    "        comb_names.append(f\"{cos2}\")\n",
    "\n",
    "        for cn in comb_names:\n",
    "            temp_names.append([cn])\n",
    "\n",
    "        # Combined representations using dictionary\n",
    "        combinations = [\n",
    "            (['Word', exp1], 'word + exp1'),\n",
    "            (['Word', exp1, cos1], 'word + exp1 + cos1'),\n",
    "            (['Word', exp2, cos1], 'word + exp2 + cos1'),\n",
    "            (['Word', exp1, exp2], 'word + exp1 + exp2'),\n",
    "            (['Word', exp1, exp2, exp3], 'word + exp1 + exp2 + exp3'),\n",
    "            (['Word', exp1, cos1, exp2], 'word + exp1 + cos1 + exp2'),\n",
    "            (['Word', exp1, exp2, cos2], 'word + exp1 + cos2 + exp2')\n",
    "        ]\n",
    "\n",
    "        for combo, mark_name in combinations:\n",
    "            # file_name = rep_dict[combo]\n",
    "            # t_list = file_name.split('_')\n",
    "            # t_list.sort()\n",
    "            temp_names.append(combo)\n",
    "\n",
    "        optimal_solutions[dataset] = temp_names\n",
    "    return optimal_solutions\n",
    "\n",
    "### Concatenation\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "optimal_space_dict = get_best_auc_for_epoch(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lRRnZZ3zD74n"
   },
   "outputs": [],
   "source": [
    "\"\"\"Load data\"\"\"\n",
    "accumulation_steps = 4\n",
    "\n",
    "def process_data_for_transformer(data_path, test_size, val_size, tokenizer, max_length, batch_size):\n",
    "    data = open(data_path, 'r', errors='replace').readlines()\n",
    "    labels = [int(row.strip().split('\\t')[0]) for row in data]\n",
    "    texts = [row.strip().split('\\t')[1] for row in data]\n",
    "    temp_text, test_text, temp_label, testing_label = train_test_split(texts, labels, test_size=test_size, random_state=42)\n",
    "    train_text, val_text, training_label, val_label = train_test_split(temp_text, temp_label, test_size=val_size, random_state=42)\n",
    "\n",
    "    train_dataset_bt = Dataset_bert(train_text, training_label, tokenizer, max_length)\n",
    "    val_dataset_bt = Dataset_bert(val_text, val_label, tokenizer, max_length)\n",
    "    test_dataset_bt = Dataset_bert(test_text, testing_label, tokenizer, max_length)\n",
    "\n",
    "    train_loader_bt = DataLoader(train_dataset_bt, batch_size=batch_size, shuffle=False)\n",
    "    val_dataset_bt = DataLoader(val_dataset_bt, batch_size=batch_size, shuffle=False)\n",
    "    test_loader_bt = DataLoader(test_dataset_bt, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader_bt, val_dataset_bt, test_loader_bt\n",
    "\n",
    "# for bench in bs:\n",
    "\n",
    "bench = 'distress'\n",
    "original_data_path = f'./sample_data_1/textagon_sample/'\n",
    "reps_folder_path = f'./sample_data_1/textagon_sample/distress_representations'\n",
    "sorted_reps = result_dict[bench][0]\n",
    "textagon_weights = result_dict[bench][1]\n",
    "\n",
    "### Get a word file with labels, combine word and raw\n",
    "word_file = open(f'{reps_folder_path}/Word.txt', 'r', errors='replace').readlines()\n",
    "raw_file = open(f'{original_data_path}/raw.txt', 'r', errors='replace').readlines()\n",
    "new_word_file = []\n",
    "for i, line in enumerate(word_file):\n",
    "    sample = word_file[i]\n",
    "    label = raw_file[i].strip().split('\\t')[0]\n",
    "    new_word_file.append(f'{label}\\t{sample}')\n",
    "with open(f'{original_data_path}/Word.txt', 'w') as f:\n",
    "    for item in new_word_file:\n",
    "        f.write(\"%s\" % item)\n",
    "\n",
    "\n",
    "print(f'================={bench}=================')\n",
    "\n",
    "\"\"\"Load parameters\"\"\"\n",
    "###### WordCNN ######\n",
    "### End-to-end Embedding ###\n",
    "emb_dim = 256  # embedding dimension\n",
    "num_filters = 400  # number of filters in conv layers\n",
    "num_classes = 2  # number of output classes\n",
    "lr_wordcnn = 1e-3\n",
    "lr_autoencoder = 1e-3\n",
    "batch_size_wc = 8\n",
    "min_frq_threshold = 2  # threshold for filtering rare words\n",
    "###### autoencoder\n",
    "encoding_dim = 100 # Specify the auto encoding dim\n",
    "### Pre-train (Word to Vec) ###\n",
    "window_size = 5  # context window size\n",
    "min_frq_threshold = 2  # threshold for filtering rare words\n",
    "EMBEDDING_DIMENSION = emb_dim\n",
    "isSkipgram = 1\n",
    "iterations = 30\n",
    "###### Bert ######\n",
    "max_length = 128\n",
    "batch_size_bt = 8\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "max_length_bert = 128\n",
    "lr_bert = 1e-5\n",
    "###### Concatenation part ######\n",
    "lr_mlp = 1e-3\n",
    "###### train and eval\n",
    "E2E = True # End-to-end embedding or word-to-vector embedding\n",
    "epochs = 10\n",
    "test_data = 0.2\n",
    "val_data_size = 1/32\n",
    "number_reps_keep = 7\n",
    "reps_train = 3\n",
    "\n",
    "train_loader_transformer, validation_loader_transformer, test_loader_transformer =  process_data_for_transformer(\n",
    "    data_path = f'{original_data_path}/Word.txt',\n",
    "    test_size = test_data,\n",
    "    val_size = val_data_size,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length = max_length_bert,\n",
    "    batch_size=batch_size_bt\n",
    ")\n",
    "\n",
    "# Create a master vocabulary for the dataset\n",
    "master_vocab = set()\n",
    "for rep in sorted_reps:\n",
    "    if rep == 'Boundaries.txt': continue\n",
    "    texts = open(f'{reps_folder_path}/{rep}.txt', 'r', errors='replace').readlines()\n",
    "    texts_tokened = [simple_tokenize(text) for text in texts]\n",
    "    for sentence in texts_tokened:\n",
    "        master_vocab.update(sentence)\n",
    "# Sort the master vocabulary and create word_to_idx mapping\n",
    "original_tokens = sorted([word for word in master_vocab if not word.isupper()], key=lambda x: (not x.isalpha(), x))\n",
    "replaced_tokens = sorted([word for word in master_vocab if word.isupper()])\n",
    "word_to_ix = {'<UNK>': 0, '<PAD>': 1}\n",
    "for i, word in enumerate(original_tokens + replaced_tokens, start=2):\n",
    "    word_to_ix[word] = i\n",
    "length_list = []\n",
    "for rep in sorted_reps:\n",
    "    if rep == 'Boundaries.txt': continue\n",
    "    texts = open(f'{reps_folder_path}/{rep}.txt', 'r', errors='replace').readlines()\n",
    "    texts_tokened = [simple_tokenize(text) for text in texts]\n",
    "    max_sentence_length = max(len(sentence) for sentence in texts_tokened)\n",
    "    length_list.append(max_sentence_length)\n",
    "length = max(length_list)\n",
    "\n",
    "rep_dict = dict()\n",
    "final_reps = []\n",
    "# print(sorted_reps)\n",
    "for rep in sorted_reps:\n",
    "    if len(final_reps) == number_reps_keep: continue\n",
    "    texts = open(f'{reps_folder_path}/{rep}.txt', 'r', errors='replace').readlines()\n",
    "    labels = open(f'{original_data_path}/Word.txt', 'r', errors='replace').readlines()\n",
    "    texts = [row.strip() for row in texts]\n",
    "    labels = [int(row.strip().split('\\t')[0]) for row in labels]\n",
    "    temp_text, test_text, temp_label, testing_label = train_test_split(texts, labels, test_size=test_data, random_state=42)\n",
    "    train_text, val_text, training_label, val_label = train_test_split(temp_text, temp_label, test_size=val_data_size, random_state=42)\n",
    "\n",
    "    train_tokens = [simple_tokenize(text) for text in train_text]\n",
    "    val_tokens = [simple_tokenize(text) for text in val_text]\n",
    "    test_tokens = [simple_tokenize(text) for text in test_text]\n",
    "\n",
    "    final_reps.append(rep)\n",
    "    train_loader, val_loader, test_loader = data_wordcnn_E2E(train_text, val_text, test_text, training_label, val_label, testing_label, word_to_ix, batch_size_wc, length)\n",
    "    rep_dict[str(rep)] = (word_to_ix, train_loader, val_loader, test_loader)\n",
    "\n",
    "optimal_names = optimal_space_dict[bench]\n",
    "best_auc = 0.0\n",
    "for rep_list in optimal_names:\n",
    "    rep_name = '_'.join([row for row in rep_list])\n",
    "    rep_length = len(rep_list)\n",
    "    word_to_ix_list = [rep_dict[item][0] for item in rep_list]\n",
    "    train_loader_list = [rep_dict[item][1] for item in rep_list]\n",
    "    val_loader_list = [rep_dict[item][2] for item in rep_list]\n",
    "    test_loader_list = [rep_dict[item][3] for item in rep_list]\n",
    "\n",
    "    if check_loader_shapes(train_loader_list) == False or check_loader_shapes(test_loader_list) == False:\n",
    "        print('check,', rep_name)\n",
    "        continue\n",
    "\n",
    "    train_loader_3D = create_3d_loader(train_loader_list)\n",
    "    val_loader_3D = create_3d_loader(val_loader_list)\n",
    "    test_loader_3D = create_3d_loader(test_loader_list)\n",
    "    batch_size, num_of_reps, max_sentence_length = list(train_loader_3D[0][0].shape)\n",
    "\n",
    "    vocab_size = len(word_to_ix)\n",
    "    max_sentence_length = 500\n",
    "\n",
    "    autoencoder = Autoencoder(vocab_size, max_sentence_length).to(device)\n",
    "    autoencoder_optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "    embedding_layer = nn.Embedding(vocab_size, max_sentence_length).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    # counter = 0\n",
    "    for epoch in range(10):\n",
    "        for inputs, _ in train_loader_3D:  # Ignore labels\n",
    "            inputs = inputs.long().to(device) # Flatten inputs and ensure float type\n",
    "            embedded_inputs = embedding_layer(inputs)\n",
    "            autoencoder_optimizer.zero_grad()\n",
    "            outputs = autoencoder(embedded_inputs)\n",
    "            # if counter == 0:\n",
    "            #     print(embedded_inputs.shape)\n",
    "            #     print(outputs.shape, embedded_inputs.shape)\n",
    "            #     counter = 1\n",
    "            loss = criterion(outputs, embedded_inputs)\n",
    "            loss.backward()\n",
    "            autoencoder_optimizer.step()\n",
    "\n",
    "    torch.save(autoencoder.state_dict(), f'{reps_folder_path}/autoencoder.pth')\n",
    "\n",
    "    model = CNN2D_E2E_AE(vocab_size=vocab_size, num_filters=num_filters, num_class=num_classes, num_channels= num_of_reps, width=max_sentence_length, auto_encoder_path=reps_folder_path).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr_wordcnn)\n",
    "    # for param in model.autoencoder.parameters():\n",
    "    #     param.requires_grad = False\n",
    "\n",
    "    config_bert = BertConfig.from_pretrained('bert-base-uncased', num_labels=num_classes)\n",
    "    model_bert = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "    optimizer_bert = torch.optim.AdamW(model_bert.parameters(), lr=lr_bert)\n",
    "\n",
    "    model_mlp = MLP(num_filters + config_bert.hidden_size, (num_filters + config_bert.hidden_size) // 2, num_classes ).to(device)\n",
    "    optimizer_mlp = torch.optim.Adam(model_mlp.parameters(), lr=lr_mlp)\n",
    "\n",
    "    print(f'start: {rep_name}')\n",
    "\n",
    "    best_auc = 0.0\n",
    "    best_preds, best_labels = None, None\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        model_bert.train()\n",
    "        model_mlp.train()\n",
    "        train_loss = 0\n",
    "        train_preds, train_targets = [], []\n",
    "        for (inputs, labels), inputs_trans in zip(train_loader_3D, train_loader_transformer):\n",
    "            optimizer.zero_grad()\n",
    "            optimizer_bert.zero_grad()\n",
    "            optimizer_mlp.zero_grad()\n",
    "\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            inputs_trans, attention_mask_trans = inputs_trans['input_ids'].to(device), inputs_trans['attention_mask'].to(device)\n",
    "\n",
    "            outputs = model(inputs, output_last_layer=True)\n",
    "            outputs_trans = model_bert(inputs_trans, attention_mask=attention_mask_trans)[0]\n",
    "            outputs_bt_cls = outputs_trans[:, 0]\n",
    "\n",
    "            combined_output = torch.cat((outputs, outputs_bt_cls), dim=1)\n",
    "            outputs_mlp = model_mlp(combined_output)\n",
    "\n",
    "            loss = F.cross_entropy(outputs_mlp, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer_bert.step()\n",
    "            optimizer_mlp.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            # _, preds = torch.max(outputs, 1)\n",
    "            preds = torch.argmax(outputs_mlp, dim=-1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "        train_loss /= len(train_loader_3D)  # average loss over the epoch\n",
    "        train_acc = (np.array(train_preds) == np.array(train_targets)).mean()  # accuracy\n",
    "        train_auc = roc_auc_score(train_targets, train_preds)  # AUC\n",
    "        train_recall = recall_score(train_targets, train_preds)  # recall\n",
    "        train_f1 = f1_score(train_targets, train_preds)  # F1\n",
    "        train_f2 = fbeta_score(train_targets, train_preds, beta=2)  # F2\n",
    "\n",
    "        model.eval()\n",
    "        model_bert.eval()\n",
    "        model_mlp.eval()\n",
    "\n",
    "        val_loss = 0\n",
    "        val_preds, val_targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for (inputs, labels), inputs_trans in zip(val_loader_3D, validation_loader_transformer):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                # if val_loss == 0: print(inputs.shape)\n",
    "                inputs_trans, attention_mask_trans = inputs_trans['input_ids'].to(device), inputs_trans['attention_mask'].to(device)\n",
    "                outputs = model(inputs, output_last_layer=True)\n",
    "                outputs_trans = model_bert(inputs_trans, attention_mask=attention_mask_trans)[0]\n",
    "                outputs_bt_cls = outputs_trans[:, 0]# Use only the [CLS] token representation from BERT and flatten WordCNN output\n",
    "                combined_output = torch.cat((outputs, outputs_bt_cls), dim=1)\n",
    "                outputs_mlp = model_mlp(combined_output)\n",
    "                loss = F.cross_entropy(outputs_mlp, labels)\n",
    "                val_loss += loss.item()\n",
    "                preds = torch.argmax(outputs_mlp, dim=-1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_targets.extend(labels.cpu().numpy())\n",
    "        val_loss /= len(val_loader_3D)  # average loss over the epoch\n",
    "        val_acc = (np.array(val_preds) == np.array(val_targets)).mean()  # accuracy\n",
    "        val_auc = roc_auc_score(val_targets, val_preds)  # AUC\n",
    "        val_recall = recall_score(val_targets, val_preds)  # recall\n",
    "        val_f1 = f1_score(val_targets, val_preds)  # F1\n",
    "        val_f2 = fbeta_score(val_targets, val_preds, beta=2)  # F2\n",
    "\n",
    "        test_loss = 0\n",
    "        test_preds, test_targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for (inputs, labels), inputs_trans in zip(test_loader_3D, test_loader_transformer):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                inputs_trans, attention_mask_trans = inputs_trans['input_ids'].to(device), inputs_trans['attention_mask'].to(device)\n",
    "\n",
    "                outputs = model(inputs, output_last_layer=True)\n",
    "                outputs_trans = model_bert(inputs_trans, attention_mask=attention_mask_trans)[0]\n",
    "                outputs_bt_cls = outputs_trans[:, 0]# Use only the [CLS] token representation from BERT and flatten WordCNN output\n",
    "                combined_output = torch.cat((outputs, outputs_bt_cls), dim=1)\n",
    "                outputs_mlp = model_mlp(combined_output)\n",
    "\n",
    "                loss = F.cross_entropy(outputs_mlp, labels)\n",
    "                test_loss += loss.item()\n",
    "                # _, preds = torch.max(outputs, 1)\n",
    "                preds = torch.argmax(outputs_mlp, dim=-1)\n",
    "                test_preds.extend(preds.cpu().numpy())\n",
    "                test_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "        test_loss /= len(test_loader_3D)  # average loss over the epoch\n",
    "        test_acc = (np.array(test_preds) == np.array(test_targets)).mean()  # accuracy\n",
    "        test_auc = roc_auc_score(test_targets, test_preds)  # AUC\n",
    "        test_recall = recall_score(test_targets, test_preds)  # recall\n",
    "        test_f1 = f1_score(test_targets, test_preds)  # F1\n",
    "        test_f2 = fbeta_score(test_targets, test_preds, beta=2)  # F2\n",
    "\n",
    "        if test_auc > best_auc:\n",
    "            best_auc = test_auc\n",
    "            best_preds = test_preds\n",
    "            best_labels = test_targets\n",
    "            stats = f'Stats: \\n' \\\n",
    "            f'Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, AUC: {train_auc:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f}, F2: {train_f2:.4f}\\n' \\\n",
    "            f'Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, AUC: {val_auc:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}, F2: {val_f2:.4f}\\n' \\\n",
    "            f'Test Loss: {test_loss:.4f}, Acc: {test_acc:.4f}, AUC: {test_auc:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}, F2: {test_f2:.4f}\\n\\n'\n",
    "            print(stats)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    # stats = f'Stats: \\n' \\\n",
    "    #         f'Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, AUC: {train_auc:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f}, F2: {train_f2:.4f}\\n' \\\n",
    "    #         f'Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, AUC: {val_auc:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}, F2: {val_f2:.4f}\\n' \\\n",
    "    #         f'Test Loss: {test_loss:.4f}, Acc: {test_acc:.4f}, AUC: {test_auc:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}, F2: {test_f2:.4f}\\n\\n'\n",
    "    # print(stats)\n",
    "\n",
    "### Remove the autoencoder file\n",
    "os.remove(f'{reps_folder_path}/autoencoder.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kWB9FfL5kgVq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"GPU is not available\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Print GPU details\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afUn6hlQD74o"
   },
   "source": [
    "Bert with embeddings --> Classification\n",
    "\n",
    "Bert with textagon --> Classification"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "aVGLByRtSyhM",
    "KF035dglD74k",
    "JylKZ6HJD74m"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "textagon",
   "language": "python",
   "name": "textagon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
