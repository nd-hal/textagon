<!DOCTYPE html>
<html>
<title>Configuration Page</title>
<link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Ubuntu+Mono" />
<style>

body, p, input {
	font-family: "Ubuntu Mono";
	font-size: 16px;
	}

</style>

<body>

	<div style="text-align: left; margin: 0 auto; width: 1080px;">

		<p style="font-weight: bold; text-decoration: underline; font-size: 30px; text-align: center;">Configuration</p>

		<form method="POST" enctype="multipart/form-data" action="/cgi-bin/handler.py">

			<p><b>Supply input file (TXT):</b>
			<input type="file" name="inputFile" id="inputFile"></p>

			<p><b>Supply lexicon file (ZIP; see notes above):</b>
			<input type="file" name="lexiconFile" id="lexiconFile"></p>

			<p><b>Supply spellchecker exclusions file (TXT; see notes below):</b>
			<input type="file" name="exclusionsFile" id="exclusionsFile"></p>

		  <p><b>Specify output file name (note: reused file names will be overwritten):</b>
		  <input type="text" name="outputFileName" value="output"></p>

			<p><b>Specify max multiprocessing cores (default: 4):</b>
		  <input type="text" name="maxCores" value="4"></p>

			<p><b>Specify max items to process (default: 0 [all]):</b>
			<input type="text" name="inputLimit" value="0"></p>

			<p><b>Specify max features per representation (default: 0 [all]):</b>
			<input type="text" name="maxFeatures" value="0"></p>

			<p><b>Specify min document frequency cutoff (default: 3):</b>
			<input type="text" name="minDF" value="3"></p>

			<p><b>Specify max Ngram depth (default: 4 [4-grams]):</b>
			<input type="text" name="maxNgram" value="4"></p>

			<p><b>Compute VADER Sentiment for booster/negation scoring:</b>
			<input type="checkbox" name="vader" checked></p>

			<p><b>Specify WN Affect level (default: 5):</b>
			<input type="text" name="wnaReturnLevel" value="5"></p>

			<p><b>Selected feature vectors (note: if none are checked, all will be returned):</b>
			Binary Count:<input type="checkbox" name="b" checked> |
			Count:<input type="checkbox" name="c"> |
			TF-IDF:<input type="checkbox" name="t"></p>

			<p><b>Compute character n-grams:</b>
			Binary Count:<input type="checkbox" name="B" checked> |
			Count:<input type="checkbox" name="C">

			<p><b>Add row index to output CSV:</b>
			<input type="checkbox" name="index"></p>

			<p><b>Remove features with zero variance:</b>
			<input type="checkbox" name="removeZeroVariance" checked></p>

			<p><b>Remove duplicate feature columns:</b>
			<input type="checkbox" name="removeDupColumns" checked></p>

			<p><b>Enable cross-category feature combiner:</b>
			<input type="checkbox" name="combineFeatures"></p>

			<p><b>Correct spelling errors for core representations:</b>
			<input type="checkbox" name="correctSpelling" checked></p>

			<p><b>Add additional spelling and error checking details to output CSV:</b>
			<input type="checkbox" name="additionalCols" checked></p>

			<p><b>Write representations to output:</b>
			<input type="checkbox" name="writeRepresentations" checked></p>

			<!--
			<p><b>Enter values (one per line):</b></p>
			<p><textarea name="values" style="width: 400px; height: 5em;"></textarea></p>
			-->

		  <p style="text-align: center;"><input type="submit"></p>

		</form>

		<div>
		<p style="font-weight: bold; text-decoration: underline; font-size: 30px; text-align: center;">Instructions</p>
		<p>Use the following list of options to configure the tool. There are a couple of key points to be aware of:</p>
			<ul>
				<li>You may supply the tool with custom lexicons by placing them into a zip file. Within the file, each lexicon should be formatted as a text file with a meaningful name (e.g., ADR.txt), since the file name will appear in the output CSV columns (e.g., LEXICONADR). Moreover, within the text files (which MUST end in .txt [otherwise ignored]), you should include a sub-category tag followed by a tab and a comma separated list of associated tokens (from which leading/trailing white space will be stripped). You may download an example lexicon set <a href="external/lexicons/ADR.zip">here</a> (add or remove files from this zip and attach below).
				<li>You may also supply a list of excluded words for the spellchecker feature. The should be attached as a text file with one word per line. Be aware that case is important for excluded words. For example, given the word 'fluoxetine' (all lowercase), the following words would be ignored: 'fluoxetine', 'Fluoxetine', and 'FLUOXETINE'. However, given the word 'Fluoxetine', only the following would be ignored: 'FLUOXETINE' and 'Fluoxetine' (but not 'fluoxetine', which would convert to the spellchecker's default correction). If in doubt, the safest approach is to use all lowercase words.
				<li>When specifying your output file name, be aware that reusing the same output file name will overwrite an existing output file with the same name. Moreover, if you click the output file on the next page before the tool has finished and used an existing file name, you may accidently download an older output file (check the time stamp on output files based on the log info to verify if necessary).
				<li>The input file should consist of one text fragment per line. An example file dvd.txt is available <a href="data/dvd.txt">here</a> (right click and save-link-as). Note that in the demo file each line starts with a numeric class label followed by a tab and then the text. If class labels are not supplied, then the program will abort with an error.
				<li>It is generally a good idea to limit the max items to process to a small number (e.g., 4) to test everything prior to running the tool since run times are often long even with multiprocessing in use.
				<li>Only one instance of the tool can be run at a time. Thus, reloading the cgi-bin page or starting a new query will reset the tool and have it start over. If this occurs, the message <text style="color: red; font-weight: bold;">"Notice: Killed outstanding multi-core processes!"</text> will appear on the log screen.
				<li>Non-English text can cause problems with Java resulting in out-of-memory heap size errors (e.g., Stanford POS Tagger). For example, in dvd.txt, line 1357 will crash with the default 1GB heap size; currently, the tagger is set to use up to 8GB per core, although this is hard-coded. Future versions will add support for memory control throughout as well as fallbacks for error handling (Java-related memory heap errors cannot be caught/handled with Python's try/except methods). Language disambiguation and result flagging is on the to-do list; currently mixed Language or non-English text will be treated as-is; some components handle this on a by-language basis where possible (e.g., the Stanford tools, such as the POS tagger support Spanish, whereas WN Affect has no support for anything but English).
				<li>This package includes a custom-written implementation of WNAffect (WNA) for Python since existing implementations do not work due to the disparity between the WN 1.6 offsets used by WNA and the WN 3.0 offsets used by nltk. This function allows for control of the affect level to return via wnaReturnLevel (default = 6; use 0 to return the word's direct parent category). The function also automatically searches for the nearest synset (up to 5 alternatives if available) for a given word that does have an associated affect since the offset relations between WN 1.6 and 3.0 are not always directly comparable (i.e., due to updates in WN 3.0 to the various synsets); there is an argument to control this search parameter but it is not exposed to this tool (yet).
				<li>For feature vectorization, scikit is used. The max features argument below limits the result to only the top X number of features, based on term frequency. The min frequency cutoff based on document frequency can be set using either an integer (absolute count) or float (proportion of documents, e.g., 0.5). Note that certain settings become meaningless if the number of documents is small. For Tfidf vectors, the following options are used: use_idf (True), smooth_idf (True), and sublinear_tf (True).
		</div>

</body>
</html>
