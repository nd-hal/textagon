{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, World!\n",
      "Error: The file './Textagon/textagon-portable/upload/Lexicons_v5.zip' does not exist.\n",
      "Please ensure that you provide the full path to the file.\n",
      "Example path looks like: C:/paht/to/zip/file/lexicon.zip\n"
     ]
    }
   ],
   "source": [
    "import textagon\n",
    "\n",
    "textagon.say_hello()\n",
    "read_lex = textagon.ReadAllLexicons(lexiconFileFullPath='G:/mendoza/analytics/Textagon/textagon-portable/upload/Lexicons_v5.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fnmatch\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import zipfile as zf\n",
    "import nltk\n",
    "import re\n",
    "import collections\n",
    "import enchant\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from typing import Tuple, List\n",
    "import pkg_resources\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm', exclude = ['lemmatizer'])\n",
    "nlp.max_length = 10 ** 10\n",
    "with SuppressStdErr():\n",
    "    import pywsd\n",
    "    from pywsd import disambiguate\n",
    "    from pywsd.lesk import adapted_lesk\n",
    "def splitWS (sentence):\n",
    "    return(sentence.split(' '))\n",
    "import numpy as np\n",
    "sys.path.append(basepath + '/external/extracted/WNAffect-master')\n",
    "from wnaffect import WNAffect\n",
    "from emotion import Emotion\n",
    "wna = WNAffect(basepath + '/external/extracted/wordnet-1.6', basepath + '/external/extracted/wn-domains')\n",
    "\n",
    "\n",
    "def ReadAllLexicons (lexiconFileFullPath):\n",
    "    def is_valid_zip_file(file_path):\n",
    "        # Check if file exists\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Error: The file '{file_path}' does not exist.\")\n",
    "            print(\"Please ensure that you provide the full path to the file.\")\n",
    "            print('Example path looks like: C:/paht/to/zip/file/lexicon.zip')\n",
    "            return False\n",
    "        # Check if file is a zip file\n",
    "        if not zf.is_zipfile(file_path):\n",
    "            print(f\"Error: The file '{file_path}' is not a valid zip file.\")\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    if not is_valid_zip_file(lexiconFileFullPath):\n",
    "        return False\n",
    "\n",
    "    customLexicons = {}\n",
    "\n",
    "    def BuildLexicon (L, customLexicons):\n",
    "\n",
    "        tagTokenPairs = list(filter(None, L.split('\\n')))\n",
    "\n",
    "        #print(tagTokenPairs)\n",
    "\n",
    "        for i, tagTokenPair in enumerate(tagTokenPairs):\n",
    "            elements = tagTokenPair.split('\\t')\n",
    "            tag = elements[0].strip().upper()\n",
    "            #print(tag)\n",
    "            #print(elements)\n",
    "            tokens = elements[1].lower().split(',')\n",
    "            tokens = [x.strip() for x in tokens]\n",
    "\n",
    "            # add every lexicon word to spell checker (not used)\n",
    "            '''\n",
    "            for each in tokens:\n",
    "                spellchecker.add(each)\n",
    "            '''\n",
    "\n",
    "            if i == 0:\n",
    "                customLexicons[os.path.splitext(os.path.basename(file))[0].upper() ] = {tag: tokens}\n",
    "            else:\n",
    "                customLexicons[os.path.splitext(os.path.basename(file))[0].upper() ][tag] = tokens\n",
    "\n",
    "        return(customLexicons)\n",
    "\n",
    "    zipFile = zf.ZipFile(lexiconFileFullPath, 'r')\n",
    "    for file in sorted(zipFile.namelist()):\n",
    "        if fnmatch.fnmatch(file, '*.txt'):\n",
    "            L = zipFile.read(file).decode('utf-8').encode('utf-8').decode('unicode-escape')\n",
    "            customLexicons = BuildLexicon(L, customLexicons)\n",
    "    print('# Custom Lexicons Imported:', len(customLexicons), '#')\n",
    "\n",
    "    # sort lexicon names alphabetically\n",
    "    customLexicons = OrderedDict(sorted(customLexicons.items()))\n",
    "\n",
    "    if len(customLexicons) != 0:\n",
    "        for key, value in customLexicons.items():\n",
    "\n",
    "            # sort lexicon tags alphabetically\n",
    "            customLexicons[key] = OrderedDict(sorted(value.items()))\n",
    "\n",
    "            print('-', key, '(' + str(len(value)) + ' Tags)')\n",
    "    print('\\r')\n",
    "\n",
    "    return(customLexicons)\n",
    "\n",
    "def SanityCheck(dataPath: str = None, override_original_file: bool = False) -> Tuple[int, dict, List[Tuple[str, str]]]:\n",
    "    print(\"Sanity check started...\")\n",
    "    def is_valid_file(file_path: str) -> bool:\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Error: The file '{file_path}' does not exist. Please ensure that you provide the full path to the file.\")\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    if not is_valid_file(dataPath):\n",
    "        return -1, {}, []\n",
    "\n",
    "    spellchecker = enchant.Dict(\"en_US\")\n",
    "    classes_counter = collections.Counter()\n",
    "    raw_data = []\n",
    "\n",
    "    with open(dataPath, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) != 2:\n",
    "            print(f\"Error: Invalid format on line '{line.strip()}', each line should contain exactly two parts separated by a tab.\")\n",
    "            print('Please make sure your data have labels in first column and text in second column.')\n",
    "            break\n",
    "\n",
    "        label, text = parts\n",
    "        classes_counter[label] += 1\n",
    "\n",
    "        words = re.findall(r'\\b\\w+\\b', text)\n",
    "        checked_text = \" \".join(word for word in words if spellchecker.check(word))\n",
    "\n",
    "        raw_data.append((label, checked_text))\n",
    "\n",
    "    num_classes = len(classes_counter)\n",
    "    samples_per_class = dict(classes_counter)\n",
    "\n",
    "    if override_original_file:\n",
    "        with open(dataPath, 'w') as f:\n",
    "            for label, text in raw_data:\n",
    "                f.write(f\"{label}\\t{text}\\n\")\n",
    "    print(f\"Sanity check completed, found {num_classes} classes and {len(raw_data)} samples.\")\n",
    "\n",
    "    ret_dict = {\n",
    "        'num_classes': num_classes,\n",
    "        'samples_per_class': samples_per_class,\n",
    "        'raw_data': raw_data\n",
    "    }\n",
    "\n",
    "    return ret_dict\n",
    "\n",
    "def ReadRawText (path: str = None):\n",
    "    print(\"Reading raw text...\")\n",
    "    pure_chunck = SanityCheck(dataPath=path, override_original_file=False)\n",
    "    classLabels = list(pure_chunck['samples_per_class'].keys())\n",
    "    raw = [x[1] for x in pure_chunck['raw_data']]\n",
    "    print(\"Reading raw text completed.\")\n",
    "    return({'corpus': raw, 'classLabels': classLabels})\n",
    "\n",
    "def setSpellChecking(exclusionsFileFullPath='None'):\n",
    "    b = enchant.Broker()\n",
    "    spellcheckerLibrary = 'en'\n",
    "    b.set_ordering(spellcheckerLibrary, 'aspell')\n",
    "\n",
    "    if exclusionsFileFullPath == 'None':\n",
    "        # Use the default exclusions file\n",
    "        exclusionsFileFullPath = pkg_resources.resource_filename('textagon', 'external/lexicons/exclusions.txt')\n",
    "    elif not os.path.isfile(exclusionsFileFullPath):\n",
    "        print('Provided exclusions file does not exist. Switching to default exclusions file.')\n",
    "        # Switch to the default exclusions file if the provided one does not exist\n",
    "        exclusionsFileFullPath = pkg_resources.resource_filename('textagon', 'external/lexicons/exclusions.txt')\n",
    "\n",
    "    try:\n",
    "        spellchecker = enchant.DictWithPWL(spellcheckerLibrary, pwl = exclusionsFileFullPath, broker = b)\n",
    "\n",
    "        exclusionsFile = open(exclusionsFileFullPath, 'r')\n",
    "        exclusionsLength = len(exclusionsFile.readlines())\n",
    "        exclusionsFile.close()\n",
    "\n",
    "        print('# Spellchecker Details #')\n",
    "        print('Provider:', spellchecker.provider)\n",
    "        print('Enchant Version:', enchant.get_enchant_version())\n",
    "        print('Dictionary Tag:', spellchecker.tag)\n",
    "        print('Dictionary Location:', spellchecker.provider.file)\n",
    "        print('Total Exclusions: ' + str(exclusionsLength))\n",
    "\n",
    "        # Return the values\n",
    "        return spellcheckerLibrary, exclusionsFileFullPath, exclusionsLength, spellchecker\n",
    "    except Exception as e:\n",
    "        print(f'Error opening or reading file {exclusionsFileFullPath}: {e}')\n",
    "        return None, None, 0, None\n",
    "\n",
    "def TextToFeatures (textData, debug = False, lexicons = None, wnaReturnLevel = 5, useSpellChecker = True, provideMisspellingDetailed = True, useCores = 1):\n",
    "\n",
    "    textData = pd.DataFrame({\n",
    "        'InitialSentence': textData\n",
    "        })\n",
    "\n",
    "    def BasicTextCleanup (sentence, debug = False):\n",
    "\n",
    "        if debug:\n",
    "            print('\\nInitial Sentence:', sentence)\n",
    "\n",
    "        # note: need to add exception handler (e.g., non-English issues)\n",
    "\n",
    "        # Basic Cleaning\n",
    "        initialSentenceLength = len(sentence)\n",
    "\n",
    "        # Strip html\n",
    "        sentence = BS(sentence, 'html.parser').get_text()\n",
    "        htmlStripLength = initialSentenceLength - len(sentence)\n",
    "\n",
    "        # Strip all excessive whitespace (after html to ensure no additional spaces result from html stripping)\n",
    "        sentence = ' '.join(sentence.split())\n",
    "        whitespaceStripLength = initialSentenceLength - htmlStripLength - len(sentence)\n",
    "\n",
    "        # Spellchecking\n",
    "        spellingCorrectionDetailsSentences = []\n",
    "        spellingCorrectionDetailsWords = []\n",
    "        spellingCorrectionDetailsSuggestions = []\n",
    "        spellingCorrectionDetailsChosenSuggestion = []\n",
    "        spellingCorrectionDetailsChangesWord = []\n",
    "        spellingCorrectionDetailsReplacementLength = []\n",
    "        spellingCorrectionCount = 0\n",
    "\n",
    "        spellchecker = enchant.DictWithPWL(spellcheckerLibrary, pwl = exclusionsFileFullPath, broker = b)\n",
    "        chkr = SpellChecker(spellchecker, sentence, filters = [EmailFilter, URLFilter])\n",
    "\n",
    "        collectMisspellingDetails = {\n",
    "            'Word': [], \n",
    "            'Substitution': [], \n",
    "            'SubstitutionText': []\n",
    "            }\n",
    "\n",
    "        for err in chkr:\n",
    "\n",
    "            #print('\\nSpellcheck Word:', err.word)\n",
    "            matchedWord = False\n",
    "\n",
    "            word = err.word\n",
    "\n",
    "            if lexicons is not None and provideMisspellingDetailed:\n",
    "\n",
    "                appendLexiconLabel = ''\n",
    "\n",
    "                for lexicon, tagTokenPairs in lexicons.items():\n",
    "\n",
    "                    lexiconName = '|_|' + lexicon.upper() + '&'\n",
    "\n",
    "                    matchedWord = False  # note: we want to capture in multiple lexicons (but only once per lexicon)\n",
    "\n",
    "                    for tag, tokens in tagTokenPairs.items():\n",
    "\n",
    "                        if matchedWord:\n",
    "                            break\n",
    "\n",
    "                        elif any('*' in s for s in tokens):\n",
    "                            # regex mode\n",
    "                            nonmatching = [s for s in tokens if not s.endswith('*')]\n",
    "                            if word.lower() in nonmatching:\n",
    "                                appendLexiconLabel += lexiconName + tag.upper()\n",
    "                                matchedWord = True\n",
    "                            else:\n",
    "                                matching = [s for s in tokens if s.endswith('*')]\n",
    "                                for eachToken in matching:\n",
    "                                    startString = eachToken[:-1]\n",
    "                                    startStringUnique = set(startString)\n",
    "                                    if startStringUnique != set('*'):\n",
    "                                        if word.lower().startswith(startString):\n",
    "\n",
    "                                            appendLexiconLabel += lexiconName + tag.upper()\n",
    "                                            matchedWord = True\n",
    "                                    else:\n",
    "                                        if eachToken == word.lower():\n",
    "\n",
    "                                            appendLexiconLabel += lexiconName + tag.upper()\n",
    "                                            matchedWord = True\n",
    "\n",
    "                        elif word.lower() in tokens:\n",
    "\n",
    "                            appendLexiconLabel += lexiconName + tag.upper()\n",
    "                            matchedWord = True\n",
    "\n",
    "                collectMisspellingDetails['SubstitutionText'].append('MISSPELLING' + appendLexiconLabel)\n",
    "\n",
    "            #print(appendLexiconLabel)\n",
    "            collectMisspellingDetails['Word'].append(err.word)\n",
    "            collectMisspellingDetails['Substitution'].append('ABCMISSPELLING' + str(len(collectMisspellingDetails['Word'])) + 'XYZ')\n",
    "\n",
    "            if (len(err.suggest()) == 0):\n",
    "                spellingCorrectionDetailsSentences.append(sentence)\n",
    "                spellingCorrectionDetailsChangesWord.append('True')\n",
    "                spellingCorrectionDetailsWords.append(err.word)\n",
    "                spellingCorrectionDetailsSuggestions.append(' | '.join(err.suggest()))\n",
    "                spellingCorrectionDetailsChosenSuggestion.append('NA')\n",
    "                spellingCorrectionDetailsReplacementLength.append('NA')\n",
    "            else: # no need to count case corrections (e.g., i'm = I'm), but go ahead and perform them\n",
    "                spellingCorrectionDetailsSentences.append(sentence)\n",
    "                spellingCorrectionDetailsWords.append(err.word)\n",
    "                spellingCorrectionDetailsSuggestions.append(' | '.join(err.suggest()))\n",
    "                if err.word.lower() != err.suggest()[0].lower():\n",
    "                    spellingCorrectionDetailsChangesWord.append('True')\n",
    "                    spellingCorrectionCount += 1\n",
    "                else:\n",
    "                    spellingCorrectionDetailsChangesWord.append('False')\n",
    "\n",
    "                finalSuggestions = err.suggest()\n",
    "\n",
    "                err.replace(finalSuggestions[0])\n",
    "                spellingCorrectionDetailsChosenSuggestion.append(finalSuggestions[0])\n",
    "                spellingCorrectionDetailsReplacementLength.append(len(finalSuggestions[0].split()))\n",
    "\n",
    "        sentenceMisspelling = sentence\n",
    "        #print('\\nRaw:', sentenceMisspelling)\n",
    "\n",
    "        for i, word in enumerate(collectMisspellingDetails['Word']):\n",
    "\n",
    "            replacementLength = spellingCorrectionDetailsReplacementLength[i]\n",
    "            # if there is no suggested replacement\n",
    "            if replacementLength == 'NA':\n",
    "                replacementLength = 1\n",
    "\n",
    "            sentenceMisspelling = re.sub('(?<=[^a-zA-Z0-9])' + word + '(?![a-zA-Z0-9])', ' '.join([collectMisspellingDetails['Substitution'][i]] * replacementLength), sentenceMisspelling, count = 1)\n",
    "\n",
    "        MisspellingRaw = ' '.join(spaCyTOK(sentenceMisspelling)).lower()\n",
    "\n",
    "        Misspelling = re.sub('ABCMISSPELLING[0-9]+XYZ'.lower(), 'MISSPELLING', MisspellingRaw)\n",
    "\n",
    "        if provideMisspellingDetailed == True:\n",
    "\n",
    "            MisspellingDetailed = MisspellingRaw\n",
    "\n",
    "            for i, word in enumerate(collectMisspellingDetails['Word']):\n",
    "\n",
    "                replacementLength = spellingCorrectionDetailsReplacementLength[i]\n",
    "                # if there is no suggested replacement\n",
    "                if replacementLength == 'NA':\n",
    "                    replacementLength = 1\n",
    "\n",
    "                MisspellingDetailed = MisspellingDetailed.replace(collectMisspellingDetails['Substitution'][i].lower(), collectMisspellingDetails['SubstitutionText'][i], replacementLength)\n",
    "\n",
    "\n",
    "            MisspellingDetailed = MisspellingDetailed\n",
    "\n",
    "        #print('\\nMISSPELLING Representation:', Misspelling)\n",
    "        #print('\\nMISSPELLINGDETAILED Representation:', MisspellingDetailed)\n",
    "\n",
    "        if useSpellChecker:\n",
    "            sentence = chkr.get_text()\n",
    "            correctedSentence = sentence\n",
    "        else:\n",
    "            correctedSentence = chkr.get_text()\n",
    "\n",
    "        #print('\\nCorrected Sentence:', correctedSentence)\n",
    "\n",
    "        checkLength = [\n",
    "            len(spellingCorrectionDetailsSentences),\n",
    "            len(spellingCorrectionDetailsWords),\n",
    "            len(spellingCorrectionDetailsChangesWord),\n",
    "            len(spellingCorrectionDetailsReplacementLength),\n",
    "            len(spellingCorrectionDetailsSuggestions),\n",
    "            len(spellingCorrectionDetailsChosenSuggestion)\n",
    "            ]\n",
    "\n",
    "        if debug:\n",
    "            print('correctionDF:', checkLength)\n",
    "\n",
    "        if not all(x == checkLength[0] for x in checkLength):\n",
    "            print('\\nProblem detected with the following text (spellchecker):', '\\n')\n",
    "            print(sentence)\n",
    "            print(spellingCorrectionDetailsSuggestions)\n",
    "            print(spellingCorrectionDetailsChosenSuggestion)\n",
    "            print(spellingCorrectionDetailsReplacementLength)\n",
    "\n",
    "        correctionDF = pd.DataFrame({\n",
    "            #'RawInput': spellingCorrectionDetailsSentences,\n",
    "            'RawWord': spellingCorrectionDetailsWords,\n",
    "            'ChangesWord': spellingCorrectionDetailsChangesWord,\n",
    "            'ReplacementLength': spellingCorrectionDetailsReplacementLength,\n",
    "            'Suggestions': spellingCorrectionDetailsSuggestions,\n",
    "            'ChosenSuggestion': spellingCorrectionDetailsChosenSuggestion\n",
    "            })\n",
    "\n",
    "        if debug:\n",
    "            print('CorrectedSentence:', correctedSentence)\n",
    "            print('CountStrippedWhitespaceChars:', whitespaceStripLength)\n",
    "            print('CountStrippedHTMLChars:', htmlStripLength)\n",
    "            print('CountSpellingCorrections', spellingCorrectionCount)\n",
    "            print(correctionDF)\n",
    "\n",
    "        resReturn = pd.DataFrame({\n",
    "            'Sentence': sentence, \n",
    "            'Feature_Misspelling': Misspelling,\n",
    "            'Spellchecker_CorrectedSentence': correctedSentence,\n",
    "            'Spellchecker_CountStrippedWhitespaceChars': whitespaceStripLength,\n",
    "            'Spellchecker_CountStrippedHTMLChars': htmlStripLength,\n",
    "            'Spellchecker_CountSpellingCorrections': spellingCorrectionCount\n",
    "            }, index = [0])\n",
    "\n",
    "        if provideMisspellingDetailed:\n",
    "            resReturn['Feature_MisspellingDetailed'] = MisspellingDetailed\n",
    "\n",
    "        return([resReturn, correctionDF])\n",
    "\n",
    "    # Basic Text Cleanup\n",
    "    print('# Performing Basic Text Cleanup #\\n')\n",
    "    res = textData['InitialSentence'].mapply(BasicTextCleanup, debug = debug)\n",
    "    resZip = list(zip(*res))\n",
    "\n",
    "    textData = pd.concat([textData, pd.concat(resZip[0], ignore_index = True)], axis = 1)\n",
    "    corrections = pd.concat(resZip[1], ignore_index = True)\n",
    "    \n",
    "    # Process Text with spaCy\n",
    "    print('\\n# Processing Text Representations #\\n')\n",
    "    def ProcessText (doc, debug = debug):\n",
    "\n",
    "        doc = nlp(doc)\n",
    "\n",
    "        all_word = []\n",
    "        all_word_lower = []\n",
    "        all_pos = []\n",
    "        all_word_pos = []\n",
    "        all_ner = []\n",
    "        all_word_ner = []\n",
    "        all_bounds = []\n",
    "\n",
    "        for token in doc:\n",
    "\n",
    "            word = token.text\n",
    "            pos = token.pos_\n",
    "\n",
    "            all_word.append(word)\n",
    "            all_word_lower.append(token.lower_)\n",
    "            all_pos.append(pos)\n",
    "            all_word_pos.append(token.lower_ + '|_|' + pos)\n",
    "\n",
    "            if token.ent_iob_ == \"O\":\n",
    "                ner = token.lower_\n",
    "                all_word_ner.append(token.lower_)\n",
    "            else:\n",
    "                ner = token.ent_type_\n",
    "                all_word_ner.append(token.lower_ + '|_|' + token.ent_type_)\n",
    "\n",
    "            all_ner.append(ner)\n",
    "\n",
    "        sents = doc.sents\n",
    "\n",
    "        for eachSent in sents:\n",
    "            sentBounds = ['-'] * len([token.text for token in eachSent])\n",
    "            sentBounds[-1] = 'S'\n",
    "            all_bounds += sentBounds\n",
    "\n",
    "        all_bounds = np.array(all_bounds)\n",
    "        all_bounds[np.where(np.array(all_word) == '|||')] = 'D'\n",
    "\n",
    "        # Vars\n",
    "        Word        = all_word_lower\n",
    "        POS         = all_pos\n",
    "        Word_POS    = all_word_pos\n",
    "        NER         = all_ner\n",
    "        Word_NER    = all_word_ner\n",
    "        Boundaries  = all_bounds\n",
    "\n",
    "        # Word Sense Disambiguation\n",
    "        tempWS = disambiguate(' '.join(all_word), algorithm = adapted_lesk, tokenizer = splitWS)\n",
    "        tempWSRaw = [x[1] for x in tempWS]\n",
    "\n",
    "        # Hypernym, Sentiment, Affect\n",
    "        Hypernym = []\n",
    "        Sentiment = []\n",
    "        Affect = []\n",
    "        Word_Sense = []\n",
    "\n",
    "        # for WNAffect\n",
    "        POSTreeBank = nltk.pos_tag(all_word)\n",
    "\n",
    "        for i, each in enumerate(Word):\n",
    "\n",
    "            try:\n",
    "                wnaRes = str(wna.get_emotion(Word[i], POSTreeBank[i][1]).get_level(wnaReturnLevel))\n",
    "                Affect.append(wnaRes.upper())\n",
    "            except:\n",
    "                Affect.append(Word[i])\n",
    "\n",
    "            if (str(tempWSRaw[i]) != 'None'):\n",
    "\n",
    "                Word_Sense.append(Word[i] + '|_|' + tempWS[i][1].name().split('.')[-1:][0])\n",
    "\n",
    "                hypernyms = tempWS[i][1].hypernyms()\n",
    "\n",
    "                if len(hypernyms) > 0:\n",
    "                    Hypernym.append(hypernyms[0].name().split('.')[0].upper())\n",
    "                else:\n",
    "                    Hypernym.append(Word[i])\n",
    "\n",
    "                swnScores = swn.senti_synset(tempWS[i][1].name())\n",
    "\n",
    "                wordSentiment = ''\n",
    "\n",
    "                if swnScores.pos_score() > 2/3:\n",
    "                    wordSentiment += 'HPOS'\n",
    "                elif swnScores.pos_score() > 1/3:\n",
    "                    wordSentiment += 'MPOS'\n",
    "                else:\n",
    "                    wordSentiment += 'LPOS'\n",
    "\n",
    "                if swnScores.neg_score() > 2/3:\n",
    "                    wordSentiment += 'HNEG'\n",
    "                elif swnScores.neg_score() > 1/3:\n",
    "                    wordSentiment += 'MNEG'\n",
    "                else:\n",
    "                    wordSentiment += 'LNEG'\n",
    "\n",
    "                Sentiment.append(wordSentiment)\n",
    "\n",
    "            else:\n",
    "                Word_Sense.append(Word[i])\n",
    "                Hypernym.append(Word[i])\n",
    "                Sentiment.append(Word[i])\n",
    "\n",
    "        res = {\n",
    "            'Feature_Word': all_word_lower,\n",
    "            'Feature_POS': all_pos,\n",
    "            'Feature_Word&POS': all_word_pos,\n",
    "            'Feature_NER': all_ner,\n",
    "            'Feature_Word&NER': all_word_ner,\n",
    "            'Feature_Boundaries': all_bounds,\n",
    "            'Feature_Affect': Affect,\n",
    "            'Feature_Word&Sense': Word_Sense,\n",
    "            'Feature_Hypernym': Hypernym,\n",
    "            'Feature_Sentiment': Sentiment,\n",
    "            }\n",
    "        \n",
    "        # Generate separate lexicon features (if available)\n",
    "        LexiconFeatures = {}\n",
    "\n",
    "        if lexicons is not None:\n",
    "\n",
    "            for lexicon, tagTokenPairs in lexicons.items():\n",
    "\n",
    "                lexiconName = 'Feature_Lexicon' + lexicon.upper()\n",
    "                LexiconFeatures[lexiconName] = []\n",
    "\n",
    "                for i, word in enumerate(Word):\n",
    "\n",
    "                    LexiconFeatures[lexiconName].append(word)\n",
    "                    wordReplaced = False\n",
    "\n",
    "                    for tag, tokens in tagTokenPairs.items():\n",
    "                        if wordReplaced:\n",
    "                            break\n",
    "                        elif any('*' in s for s in tokens):\n",
    "                            # regex mode\n",
    "                            nonmatching = [s for s in tokens if not s.endswith('*')]\n",
    "                            if word.lower() in nonmatching:\n",
    "                                LexiconFeatures[lexiconName][i] = tag.upper()\n",
    "                                wordReplaced = True\n",
    "                            else:\n",
    "                                matching = [s for s in tokens if s.endswith('*')]\n",
    "                                for eachToken in matching:\n",
    "                                    startString = eachToken[:-1]\n",
    "                                    startStringUnique = set(startString)\n",
    "                                    if startStringUnique != set('*'):\n",
    "                                        if word.lower().startswith(startString):\n",
    "                                            LexiconFeatures[lexiconName][i] = tag.upper()\n",
    "                                            matchedWord = True\n",
    "                                    else:\n",
    "                                        if eachToken == word.lower():\n",
    "                                            LexiconFeatures[lexiconName][i] = tag.upper()\n",
    "                                            matchedWord = True\n",
    "\n",
    "                        elif word.lower() in tokens:\n",
    "\n",
    "                            LexiconFeatures[lexiconName][i] = tag.upper()\n",
    "                            wordReplaced = True\n",
    "\n",
    "        if lexicons is not None:\n",
    "            res.update(LexiconFeatures)\n",
    "\n",
    "        checkLength = [len(res[each]) for each in res]\n",
    "\n",
    "        if len(set(checkLength)) != 1:\n",
    "            print('Check Length:', checkLength)\n",
    "            print('Problem detected with the following text:')\n",
    "            print(sentence)\n",
    "\n",
    "        # Rejoin features\n",
    "        for each in res.keys():\n",
    "            res[each] = ' '.join(res[each])\n",
    "\n",
    "        return(res)\n",
    "\n",
    "    res = textData['Sentence'].mapply(ProcessText)\n",
    "    textData = pd.concat([textData, pd.DataFrame(res.values.tolist())], axis = 1)\n",
    "\n",
    "    return([textData, corrections])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading raw text...\n",
      "Sanity check started...\n",
      "Sanity check completed, found 2 classes and 10 samples.\n",
      "Reading raw text completed.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TextToFeatures' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m ret \u001b[39m=\u001b[39m ReadRawText(path\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mG:/mendoza/analytics/Textagon/textagon-portable/upload/test_db.txt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m data \u001b[39m=\u001b[39m ret[\u001b[39m'\u001b[39m\u001b[39mcorpus\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m----> 7\u001b[0m ttf \u001b[39m=\u001b[39m TextToFeatures(textData\u001b[39m=\u001b[39mdata)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TextToFeatures' is not defined"
     ]
    }
   ],
   "source": [
    "# lex = ReadAllLexicons(lexiconFileFullPath = 'G:/mendoza/analytics/Textagon/textagon-portable/upload/Lexicons_v5.zip')\n",
    "# ret = SanityCheck(\n",
    "#     dataPath='G:/mendoza/analytics/Textagon/textagon-portable/upload/test_db.txt',\n",
    "#     override_original_file=False)\n",
    "ret = ReadRawText(path='G:/mendoza/analytics/Textagon/textagon-portable/upload/test_db.txt')\n",
    "data = ret['corpus']\n",
    "ttf = TextToFeatures(textData=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'corpus': ['The ends the story in this way because it represents the new that will take place It s almost as if she s saying that she will hibernate during the winter and when spring returns and everything begins a new cycle of life then so will she realized that many of the things that she had thought of as strange before had become through of season upon season almost familiar to her now She is saying that when the new season begins it will be her beginning and she ll have a different outlook on her new life',\n",
       "  'Almost every time you need to be patient when you are at the doctors or in the hospital What I do to pass time when I m at the doctors is play I spy I play it with my mom every time There is usually a lot of people at the doctors when I go There is magazines but I think they are boring Therefore I play I spy to stay patient Otherwise I would be very very bored',\n",
       "  'Everyone is patient one time in their life I remember I was every patient one day when I was ten year old It was all because of popcorn A small little popcorn in my That was an day but I was very patient One day about two year ago when I was ten years old swallowed a popcorn after going to a movie I remember eating one last piece of popcorn from the popcorn box then trowing the box away Well right as soon as I and walked out the door and eaten my popcorn did it start to bother me My mom and I waked in the target nearby to get some supplies and do some errands I started to get really thirsty because of the popcorn I asked my mom if she would buy me a drink because of how thirsty was but she said she only had a credit card That meant she could by her things but she could not buy a pop I understood So I just tried not to think about it This was very hard Soon we pulled up to our next the bank My mom needed to get some money to pay off my tennis I asked her if she could please hurry because the popcorn and lack of thirst was hurting me She said she was going as fast as she could and that we were only a few minutes away from home I knew that we were more like fifteen minutes away not just a few but I t argue The and thirst was starting to get to me It felt like it had been three hours but it had only been fifteen minutes by the time we finally got home I threw the door open and ran inside to get some water I ran and opened the fridge opened a bottle of water and hugged the whole thing down Then I went outside to help my mom bring in the supplies and things then drank more water with a smile In conclusion we all to be patient sometimes I was patient to get home and drink some water I am always patient and when it pays off I am always happy',\n",
       "  'As an AI language model I do not have a personal connection to any specific country or location However I can provide information and insights on various locations around the world including those described in literature such as Heights by Emily In this article I will compare one of the places described in Heights with a place in the United Kingdom Heights is a novel set in the moors of Yorkshire England in the late 18th and early 19th centuries The novel describes the landscape as wild rugged and often bleak with the moors serving as a symbol of the harsh and unforgiving nature of the characters lives One location that stands out in the novel is Heights itself the isolated farmhouse that serves as the setting for much of the story Heights is described as a dark and foreboding place with a moody atmosphere that matches the personalities of its inhabitants The house is situated on a hill surrounded by wild heath and rocky crags and is battered by the harsh winds that blow across the moors The building itself is made of dark rough stone with small shuttered windows that seem to hide the secrets of its inhabitants from the outside world Comparing this location to a place in my own country I can think of the Scottish Highlands The Scottish Highlands are a region of Scotland that is known for its rugged mountainous terrain and its wild untamed landscapes Like the moors of Yorkshire the Scottish Highlands can be a harsh and unforgiving place with the weather often changing rapidly and unexpectedly One location in the Scottish Highlands that shares many similarities with Heights is the ruined castle of Like Heights is situated on a rocky outcrop surrounded by rugged mountains and wild untamed countryside The castle is made of rough stone and is battered by the strong winds that blow across the Like the inhabitants of Heights the people who lived in were tough and resilient able to survive in a harsh and unforgiving landscape Although the castle is now in ruins it still stands as a testament to the strength and endurance of those who lived in the Scottish Highlands in the past In conclusion the landscape of Heights is a symbol of the harsh and unforgiving nature of the characters lives Although the moors of Yorkshire and the Scottish Highlands are separated by many miles they share many similarities including their rugged wild terrain and their ability to shape the lives of those who live there Whether in a novel or in real life these landscapes remind us of the power of nature and its ability to shape our lives in unexpected ways',\n",
       "  'The mood created by the author in the memoir was grateful happy and thankful The author was grateful because he t asked to be born in hes parents to so that can live a better life was happy because he get to see both of his parents his grandparents which lives in the basement of his house and his aunts Bertha and Juanita and his cousins Arnold Maria and Rosemary was thankful because he gets a home where he can sleep at a family who loves him and he could ask for nothing better than the life he has now',\n",
       "  'The features of the setting affect the cyclist by how much confidence in where he s going and how he s get there Having no water',\n",
       "  'Cry Freedom is a historical novel based on the true story of South African anti apartheid activist Steve Biko and his relationship with white journalist Donald Woods The book portrays the struggle against the apartheid regime in South Africa including the human rights abuses and violence committed by the government against the black population The book is a powerful condemnation of racism injustice and oppression and it sheds light on an important moment in South African history In terms of literary quality Cry Freedom is well written with engaging characters vivid descriptions and a compelling narrative uses multiple perspectives to tell the story alternating between the voices of Biko and Woods which allows the reader to see the events from different angles The book is also well researched and incorporates real events and historical figures into the story making it both informative and entertaining While Cry Freedom was not a recipient of a major literary prize it has received critical acclaim and has been adapted into a successful film The book s portrayal of the struggle against apartheid is an important contribution to literature and it has helped to raise awareness about the fight for human rights in South Africa It is a powerful example of how literature can be used to expose injustice and promote social change In conclusion while I am not able to award a literary prize myself I would say that Cry Freedom is a worthy contender for such an honor The book s strong writing compelling story and important message make it a valuable contribution to literature and a powerful tool for promoting social justice',\n",
       "  'The features of the setting made the cyclist s journey even longer and more strenuous The reason it being in such a remote area did not help For example the people who still lived in that remote area were probably as old as the cyclist grandparents and the directions were horrible Also the fact that their were no new shops or areas or rest for the cyclist to go to the water pump he found was old and nasty and the only building he saw was abandoned Also factor was the rough terrain It being the cyclist was already very fatigued by lack of water and tiredness the terrain was long bumpy and just horrible all together Finally the last factor was where he was bicycling in California on a summer day and he being on a which had no n c but which bike does did not help at all These are how the features affect the cyclist journey',\n",
       "  'The author conclude this saying it to the story because he thought it would go because at about how the a yet budding in the winter and how all at the geese will return to the home and stay there because they hate the winter',\n",
       "  'Sure here s my revised answer My most unforgettable birthday was my 21st I celebrated it with a group of close friends at a cozy bar in the city The atmosphere was perfect with dim lighting and smooth jazz music What made it so memorable was the surprise gifts and thoughtful gestures from my friends They planned a scavenger hunt throughout the city with clues and challenges that led me to various locations that held special memories for us At each location there was a small gift waiting for me and I was deeply touched by the effort and thought that went into each one As the night wore on we made our way back to the bar where my friends had arranged for a cake to be brought out As we sang Happy Birthday I felt overwhelmed with gratitude for the people in my life who had made this birthday so unforgettable Looking back it was a birthday filled with warmth and joy It reminded me of the importance of friendship and how the little things can make a big impact'],\n",
       " 'classLabels': ['1', '0']}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mendoza",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
